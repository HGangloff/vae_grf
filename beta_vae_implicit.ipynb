{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0081623-ddf7-4eea-9904-1237d1385270",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5515a157-74b4-423c-9910-643c6ef5898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import jax\n",
    "from jax import jit, vmap\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from math import prod\n",
    "from jax.experimental import jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5c520e-e429-453d-b493-0e01d7c4bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"0.95\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b108b2a0-1b13-4611-ae2e-19ad8c80f8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c277e07d-1e90-4118-bc91-1dca1cf02b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ab576d-3fc4-4f49-a779-8cb123209ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 17:57:01.032513: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9197164e-9ae0-4615-8582-9b93f5c06ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0d50ae-425f-4f88-b4f9-0520b6ef759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae_jax import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ac78f0-2536-41d9-af3e-bad8f01055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "batch_size = 2\n",
    "latent_img_size = 32\n",
    "z_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb27bf5-75c3-4fe7-a89f-8943e2cc6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, vae_states = eqx.nn.make_with_state(VAE)(img_size, latent_img_size, z_dim, key)\n",
    "init_vae_params, vae_static = eqx.partition(vae, eqx.is_inexact_array)\n",
    "# x = jnp.zeros((1, 3, 256, 256)) # we must have a batch_dim, the model can only be applied afetr vmap because of BN layer\n",
    "# batch_model = jax.vmap(model, in_axes=(0, None, None, None), out_axes=(0, None), axis_name=\"batch\")\n",
    "# x, state = batch_model(x, state, key, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b136a3-1b10-4b5f-a358-87477ffcf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_train_dataloader, get_test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49666ec-6a84-4ad1-9bda-87d1a0eca25d",
   "metadata": {},
   "source": [
    "**Note:** if we were doing this gradient based search of beta without using a validation dataset for the outer loss, should we expect to have a beta estimated to be 0 as it would be optimal to overfit and kill the regularization term right ? -> this seems to be the case emprically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2be38c-7611-4f9b-a4b7-d6f311bf79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import LivestockTrainDataset\n",
    "train_dataset = LivestockTrainDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1500,\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e95dca11-5b50-4318-b670-e7ac3cf852b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LivestockTrainDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1500,\n",
    "    offset_idx=train_dataset.__len__()\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aafa604-c820-494f-8233-ff958c0b6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(2, 6))\n",
    "def loss(params, beta, static, states, x, key, train=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        XXX\n",
    "    static\n",
    "        XXX\n",
    "    states\n",
    "        XXX\n",
    "    x\n",
    "        A batch of inputs\n",
    "    key\n",
    "        A JAX PRNGKey\n",
    "    \"\"\"\n",
    "    vae = eqx.combine(params, static)\n",
    "    if train:\n",
    "        # make sure we are in training mode\n",
    "        vae = eqx.nn.inference_mode(vae, value=False)\n",
    "    else:\n",
    "        vae = eqx.nn.inference_mode(vae)\n",
    "    batched_vae = vmap(vae, in_axes=(0, None, None, None), out_axes=(0,  None, 0, 0), axis_name=\"batch\")\n",
    "\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "\n",
    "    x_rec, states, mu, logvar = batched_vae(x, states, key, train)\n",
    "    batched_elbo = vmap(vae.elbo, in_axes=(0, 0, 0, 0, None), out_axes=(0, 0, 0))\n",
    "\n",
    "    elbo, rec_term, kld = batched_elbo(x_rec, x, mu, logvar, beta)\n",
    "\n",
    "    elbo = jnp.mean(elbo) # avg over the batches\n",
    "    rec_term = jnp.mean(rec_term)\n",
    "    kld = jnp.mean(kld)\n",
    "\n",
    "    x_rec = VAE.mean_from_lambda(x_rec)\n",
    "\n",
    "    # elbo = jnp.array(0.)\n",
    "    # rec_term = jnp.array(0.)\n",
    "    # kld = jnp.array(0.)\n",
    "    # x_rec = jnp.array(0.)\n",
    "    # logvar = jnp.array(0.)\n",
    "    # mu = jnp.array(0.)\n",
    "\n",
    "    return -elbo, (x_rec, rec_term, kld, states, mu, logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fbc4f-f9d6-44f7-a5ce-d485dd10fa21",
   "metadata": {},
   "source": [
    "Test the loss on one mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958301ff-6d72-41e2-a7cc-3403972b11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = next(iter(train_dataloader))\n",
    "#loss_value, (_, _, _, vae_states, _, _) = loss(init_vae_params, 1., vae_static, vae_states, mini_batch[0].numpy(), key, train=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e6587ff-424f-4877-8405-318f67c1212d",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bfb534d-0408-4a26-8178-5754202e4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "init_params = init_vae_params\n",
    "init_states = vae_states\n",
    "static = vae_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6cdcf9e-c250-4d95-8e81-ce1316cdb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = eqx.combine(init_params, static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af1394fd-26b4-480b-86b5-3d47e08001d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "n_steps_inner = 30\n",
    "n_steps_outer = 100\n",
    "\n",
    "lr_inner = 1e-3\n",
    "lr_outer = 1e-2\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "init_beta = jnp.array(0.1)\n",
    "\n",
    "optimizer_inner = optax.adam(lr_inner)\n",
    "opt_state_inner = optimizer_inner.init(init_params)\n",
    "optimizer_outer = optax.adam(lr_outer)\n",
    "opt_state_outer = optimizer_outer.init(init_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "548bb682-1bad-4235-a474-6302c0f67cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_inner(params, beta, static, states, train_data, opt_state, key, print_every):\n",
    "    \"\"\"\n",
    "    must not necesarily be jittable since we a priori only need to get grad\n",
    "    but we provide a jittable train_inner for speed\n",
    "    \"\"\"\n",
    "\n",
    "    def make_step(carry, x):\n",
    "        params, beta, states, opt_state, key = carry\n",
    "        key, subkey = jax.random.split(key)\n",
    "        (minus_elbo, aux_loss), grads = jax.value_and_grad(loss, argnums=0, has_aux=True)(params, beta, static, states, x, subkey, train=True)\n",
    "        _, rec_term, kld, states, _, _ = aux_loss\n",
    "        updates, opt_state = optimizer_inner.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return (params, beta, states, opt_state, key), jnp.array([minus_elbo, rec_term, kld])\n",
    "\n",
    "    carry_init = (\n",
    "        params, beta, states, opt_state, key\n",
    "    )\n",
    "    carry, losses = jax.lax.scan(\n",
    "        make_step,\n",
    "        carry_init,\n",
    "        train_data\n",
    "    )\n",
    "    params = carry[0]\n",
    "    states = carry[2]\n",
    "    opt_state = carry[3]\n",
    "    return params, beta, states, (losses[:, 0], losses[:, 1], losses[:, 2]), opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f8c8a4f-a684-4ddd-9267-2ec00fed519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.custom_jvp, nondiff_argnums=(0, 2, 3, 4, 5, 6)) # we will not differentiate wrt something else than beta\n",
    "def train_inner_(params, beta, static, states, inner_train_data, opt_state_inner, key):\n",
    "    return train_inner(params, beta, static, states, inner_train_data, opt_state_inner, key, print_every)\n",
    "    \n",
    "@train_inner_.defjvp\n",
    "def train_inner_jvp(params0, static, states, x_data, opt_state_inner, key, primals, tangents):\n",
    "    \"\"\"\n",
    "    Note that nondiff arg placed at the start of the signature of the corresponding JVP rule\n",
    "    \"\"\"\n",
    "    print(\"here\")\n",
    "    beta0, = primals\n",
    "    v, = tangents\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # # compute x_0(beta_0) (for a given beta_0). The gives the couple (x0, beta_0) around which we are authorized to use th IFT formula\n",
    "    # params_0_beta, _, _, (_, _, _), _ = train_inner_(\n",
    "    #     params, beta, static, states, inner_train_data, opt_state_inner, subkey\n",
    "    # )\n",
    "    # subkeys not considered below since train=False\n",
    "    Ax = lambda x:-jax.jacfwd(lambda params_:jax.grad(loss, argnums=0, has_aux=True)(params_, beta0, static, states, x_data, subkey, train=False)[0])(params0) @ x\n",
    "    b = jax.jacfwd(lambda beta_:jax.grad(loss, argnums=0, has_aux=True)(params0, beta_, static, states, x_data, subkey, train=False)[0])(beta0) # second diff wrt lambda\n",
    "    #b = jax.jacfwd(lambda beta_:jax.grad(loss, argnums=0, has_aux=True)(params0, beta_, static, states, x_data, subkey, train=False))(b'0')# second diff wrt lambda\n",
    "\n",
    "    return params0, jax.scipy.sparse.linalg.cg(Ax, b) * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1e8f20b-fb99-4002-9d1d-6b76b55ec371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_outer(n_steps_outer, n_steps_inner, params, beta, static, states, train_loader, val_loader, opt_state_outer, opt_state_inner, key, print_every):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def infinite_val_loader():\n",
    "        while True:\n",
    "            yield from val_loader\n",
    "\n",
    "    def make_step(outer_val_batch, params, beta0, states, inner_train_data, opt_state_outer, opt_state_inner, key):\n",
    "        key, subkey = jax.random.split(key)\n",
    "\n",
    "        # this call to train_inner_ gives a x_0, theta_0 (the resulting couple (params, beta)\n",
    "        # this couple is a root of F(=dELBO(params, beta)/dparams) (for a fixed theta_0 (beta)),\n",
    "        # using a SGD that starts at a x_init (params)\n",
    "        params0, _, states, (elbo_list, rec_term_list, kld_list), opt_state_inner = train_inner_(\n",
    "            params, beta0, static, states, inner_train_data, opt_state_inner, subkey\n",
    "        )\n",
    "        # ... around this x_0, theta_0 we know that we have the right to compute dx*(theta)/dtheta and we have done so\n",
    "        # in the jvp defined in the previous cells\n",
    "\n",
    "        # key, subkey = jax.random.split(key)\n",
    "        # grads_inner = jax.jacfwd(train_inner_, argnums=1)(params, beta, static, states, x, opt_state_inner, subkey)\n",
    "\n",
    "\n",
    "        # Reconstruct the gradient here ?\n",
    "        # Ax = lambda x:-(jax.jacfwd(\n",
    "        #         lambda params_:jax.grad(\n",
    "        #             loss, argnums=0, has_aux=True\n",
    "        #         )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        #     )(params0) @ x)\n",
    "        # def mul_(x, y):\n",
    "        #     print(x)\n",
    "        #     print(y)\n",
    "        #     return x @ y\n",
    "        # Ax  = lambda x:jax.tree.map(mul_, #lambda x_, y:x_ @ y,\n",
    "        #         x,\n",
    "        #         jax.jacfwd(\n",
    "        #             lambda params_:jax.grad(\n",
    "        #                 loss, argnums=0, has_aux=True\n",
    "        #             )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        #         )(params0),\n",
    "        #         is_leaf=lambda x_: eqx.is_inexact_array(x_))\n",
    "        # Ax = \n",
    "\n",
    "        # def Ax(x):\n",
    "        #     # trying jax.jet with https://github.com/google/jax/discussions/9598\n",
    "\n",
    "        #     loss_ = lambda p: loss(p, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        #     def loss_wrapper(*true_params, tree):\n",
    "        #         \"\"\"To be able to differentiate the certain elements of the pytree\n",
    "        #         https://github.com/google/jax/discussions/12765\n",
    "\n",
    "        #         true_params is a list of the arrays with respect to which we want to differentiate inside the eqx.Module\n",
    "        #         defined by params0\n",
    "\n",
    "        #         Note that all that goes after *args are keyword only arguments\n",
    "        #         \"\"\"\n",
    "        #         p = jax.tree.unflatten(tree, true_params)\n",
    "        #         # p = jax.tree_util.tree_map_with_path(\n",
    "        #         #     lambda kp, p: dict_path_true_params[kp], # kp will nec be in dict_path_true_params because of the following is_leaf function\n",
    "        #         #     params0,\n",
    "        #         #     is_leaf=eqx.is_inexact_array\n",
    "        #         # )\n",
    "        #         return loss_(p)\n",
    "\n",
    "        #     # params0_fl = jax.tree.map(\n",
    "        #     #     lambda x_: x_.flatten(),\n",
    "        #     #     params0,\n",
    "        #     #     is_leaf=eqx.is_inexact_array\n",
    "        #     # )\n",
    "        #     true_params, tree = jax.tree_util.tree_flatten(params0) # in the children non static content (param_fl) only the jnp arrays go\n",
    "        #     # all None and integer constant go to tree so a tree_flatten suffices to get the true params\n",
    "        #     # we really optimize upon\n",
    "            \n",
    "\n",
    "        #     # true_params_identity_filled = jax.tree.map(\n",
    "        #     #     lambda x_:jnp.eye(x_.shape[0]),\n",
    "        #     #     true_params,\n",
    "        #     # )\n",
    "        #     # print(true_params)\n",
    "        #     # jet.jet(loss_, (true_params,), ((true_params_identity_filled,)))[1][0]\n",
    "        #     # fs\n",
    "        #     # # jet.jet(fun, (x,), ((v, jnp.zeros_like(x)),))[1][1]\n",
    "        #     # #     return jnp.sum(hvv(jnp.eye(x.shape[0], dtype=x.dtype)))\n",
    "            \n",
    "        #     # jet.jet(loss_, (params0,), )[1][1]\n",
    "\n",
    "        #     # # dL_dtheta = lambda params_:jax.grad(\n",
    "        #     # #         loss, argnums=0, has_aux=True\n",
    "        #     # #     )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "            \n",
    "            \n",
    "        #     d2L_dtheta2 = [\n",
    "        #         jax.jacfwd(\n",
    "        #             lambda *params_:jax.grad(loss_wrapper, argnums=i)(*params_, tree=tree),\n",
    "        #             argnums=i\n",
    "        #         )(*true_params)\n",
    "        #         for i in range(len(true_params))\n",
    "        #     ]\n",
    "        #     print(\"A\", d2L_dtheta2)\n",
    "        #     d2L_dtheta2_pytree = jax.tree.unflatten(tree, d2L_dtheta2)\n",
    "        #     res = jax.tree.map(\n",
    "        #         lambda x_, y: (y.reshape((prod(x_.shape), prod(x_.shape))) @ x_.flatten()).reshape(x_.shape),\n",
    "        #         x,\n",
    "        #         d2L_dtheta2_pytree,\n",
    "        #         is_leaf=eqx.is_inexact_array\n",
    "        #     )\n",
    "        #     print(\"B\", res)\n",
    "        #     return res\n",
    "            # # for each inexact field (x_) of the VAE params we only compute the @ (and let a non None value)\n",
    "            # # with the array (y_) at the same position of the nested VAE params located (at the x_ inexact field of the outer VAE params)\n",
    "            # # this can be seen as retrieving the diagonal of the Hessian matrix\n",
    "            # nested_vaes = jax.tree_util.tree_map_with_path(\n",
    "            #     lambda key_path_x_, x_, y: jax.tree_util.tree_map_with_path(\n",
    "            #         lambda key_path_y_, y_: (y_.reshape((prod(x_.shape), prod(x_.shape))) @ x_.flatten()).reshape(x_.shape) if key_path_x_ == key_path_y_ else None,\n",
    "            #         y,\n",
    "            #         is_leaf=eqx.is_inexact_array\n",
    "            #     ),\n",
    "            #     x,\n",
    "            #     pytree_grad,\n",
    "            #     is_leaf=eqx.is_inexact_array\n",
    "            # )\n",
    "                \n",
    "            # outer_vae = jax.tree_util.tree_map_with_path(\n",
    "            #     lambda key_path_x_, x_, y: jax.tree.leaves(y, is_leaf=eqx.is_inexact_array)[0],\n",
    "            #     x,\n",
    "            #     nested_vaes,\n",
    "            #     is_leaf=eqx.is_inexact_array\n",
    "            # )\n",
    "            # #outer_vae = jax.tree.leaves(nested_vaes, is_leaf=eqx.is_inexact_array)\n",
    "            # #print(outer_vae)\n",
    "                \n",
    "            # #print(outer_vae)\n",
    "            # return outer_vae\n",
    "                \n",
    "            \n",
    "        #b = jax.jacfwd(lambda beta_:jax.grad(loss, argnums=0, has_aux=True)(params0, beta_, static, states, outer_val_batch, subkey, train=False)[0])(beta0)\n",
    "        \n",
    "        true_params, tree = jax.tree_util.tree_flatten(params0)\n",
    "        nb_params_array = len(true_params)\n",
    "\n",
    "        loss_ = lambda p: loss(p, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        def loss_wrapper(*true_params, tree):\n",
    "            \"\"\"To be able to differentiate the certain elements of the pytree\n",
    "            https://github.com/google/jax/discussions/12765\n",
    "\n",
    "            true_params is a list of the arrays with respect to which we want to differentiate inside the eqx.Module\n",
    "            defined by params0\n",
    "\n",
    "            Note that all that goes after *args are keyword only arguments\n",
    "            \"\"\"\n",
    "            p = jax.tree.unflatten(tree, true_params)\n",
    "            return loss_(p)\n",
    "\n",
    "        # Note that we cannot have tracer for grad's argnum (https://stackoverflow.com/questions/77913154/vectorizing-power-of-jax-grad)\n",
    "        @partial(jax.jit, static_argnums=(1,))\n",
    "        def get_gradients(idx, nb_params_array):\n",
    "            \"\"\"\n",
    "            Returns the idx gradient among the nb_params_array gradients that are precomputed at trace time\n",
    "            \"\"\"\n",
    "            d2L_dtheta2_ = []\n",
    "            d2L_dthetadbeta_ = []\n",
    "            for i in range(nb_params_array):\n",
    "                d2L_dtheta2_.append(\n",
    "                    lambda _: jax.jacfwd(\n",
    "                        lambda *params_:jax.grad(loss_wrapper, argnums=i)(*params_, tree=tree),\n",
    "                        argnums=i\n",
    "                    )(*true_params)\n",
    "                )\n",
    "                d2L_dthetadbeta_.append(\n",
    "                    lambda _:jax.jacfwd(lambda beta_:jax.grad(loss_wrapper, argnums=i)(*true_params, tree=tree))(beta0)\n",
    "                )\n",
    "            return (jax.lax.switch(idx, d2L_dtheta2_, ()), jax.lax.switch(idx, d2L_dthetadbeta_, ()))\n",
    "        \n",
    "        def get_gradients_wrapper(idx):\n",
    "            return get_gradients(idx, nb_params_array)\n",
    "\n",
    "        def scan_fun(_, i):\n",
    "            d2L_dtheta2_i, d2L_dthetadbeta_i = get_gradients_wrapper(i)\n",
    "            print(d2L_dtheta2_i.shape, d2L_dthetadbeta_i.shape)\n",
    "            def Ax(x):\n",
    "                return (d2L_dtheta2_i.reshape((prod(x.shape), prod(x.shape))) @ x.flatten()).reshape(x.shape)\n",
    "            grads = jax.scipy.sparse.linalg.cg(Ax, d2L_dthetadbeta_i)\n",
    "            return None, grads\n",
    "\n",
    "        _, grads_inner = jax.lax.scan(\n",
    "            scan_fun,\n",
    "            None,\n",
    "            jnp.arange(nb_params_array)\n",
    "        )\n",
    "        print(grads_inner)\n",
    "        fs\n",
    "\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # Now we want to compute the ELBO for the outer loss and take gradient wrt to theta here.\n",
    "        # We have defined a rule to backpropagate through beta i.e.\n",
    "        # through train_inner (dx*(theta)/dtheta) in the previous cells\n",
    "        (minus_elbo, aux_loss), grads_outer = jax.value_and_grad(loss, argnums=1, has_aux=True)(params, beta, static, states, outer_val_batch, subkey, train=False)\n",
    "        x_rec, rec_term, kld, _, _, _ = aux_loss\n",
    "        print(grads_outer.shape)\n",
    "        grads = grads_outer @ grads_inner\n",
    "        print(grads.shape)\n",
    "        updates, opt_state_outer = optimizer_outer.update(grads, opt_state_outer, beta)\n",
    "        beta = optax.apply_updates(beta, updates)\n",
    "        return params, beta, states, opt_state_outer, opt_state_inner, key, x_rec, (elbo_list, rec_term_list, kld_list)\n",
    "\n",
    "    elbo_list = []\n",
    "    rec_term_list = []\n",
    "    kld_list = []\n",
    "    beta_list = []\n",
    "\n",
    "\n",
    "    for step, (x, _) in zip(range(n_steps_outer), infinite_val_loader()): \n",
    "        x = x.numpy()\n",
    "        inner_train_data = jnp.asarray(\n",
    "            list(map(lambda x:x[0].numpy(), list(itertools.islice(train_loader, n_steps_inner)))) # get the next n_steps_inner elements from train_loader\n",
    "        )\n",
    "        params, beta, states, opt_state_outer, opt_state_inner, key, x_rec, losses = make_step(\n",
    "            x, params, beta, states, inner_train_data, opt_state_outer, opt_state_inner, key\n",
    "        )\n",
    "        elbo_list.extend(-losses[0])\n",
    "        rec_term_list.extend(losses[1])\n",
    "        kld_list.extend(losses[2])\n",
    "        # elbo_list.append(-losses[0])\n",
    "        # rec_term_list.append(losses[1])\n",
    "        # kld_list.append(losses[2])\n",
    "        beta_list.append(jnp.full((n_steps_inner,), beta))\n",
    "        \n",
    "        if (step % print_every) == 0 or (step == n_steps - 1):\n",
    "            print(\n",
    "                f\"{step=}, elbo_loss={elbo_list[-1]}, rec_term={rec_term_list[-1]}, kld_term={kld_list[-1]}, beta={beta_list[-1][0]}\"\n",
    "            )\n",
    "            \n",
    "    return params, beta, states, (elbo_list, rec_term_list, kld_list), (opt_state_outer, opt_state_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adcfb805-5c54-42fb-9b83-4e7845ff23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.tree.map(lambda x, y: jax.tree.map(lambda y_:x+y_, y, is_leaf=lambda x:(isinstance(x, float))),\n",
    "#              (0., 0., 0.),\n",
    "#              ((1., None), (2., None), (3., None, 4)),\n",
    "#              is_leaf=lambda x:(isinstance(x, float)))# and isinstance(y, tuple)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a810f-bd72-4da6-b317-5f9c555c9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32) (32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 17:58:56.966691: E external/xla/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %reduce-window.28 = f32[2,1,64,64,64]{4,3,2,1,0} reduce-window(f32[2,1,128,128,64]{4,3,2,1,0} %constant.20437, f32[] %constant.2652), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x1_1x1_1x0_0}, to_apply=%region_8.31595, metadata={op_name=\"jit(scan)/jit(main)/while/body/jit(get_gradients)/cond/branch_1_fun/vmap(jvp(jvp(jit(loss))))/vmap(eqx.nn.MaxPool2d)/eqx.nn.Pool/reduce_window_max[window_dimensions=(1, 1, 3, 3, 1) window_strides=(1, 1, 2, 2, 1) padding=((0, 0), (0, 0), (1, 1), (1, 1), (0, 0)) base_dilation=(1, 1, 1, 1, 1) window_dilation=(1, 1, 1, 1, 1)]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/resnet.py\" source_line=357}\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2024-06-12 17:58:57.247500: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.281037089s\n",
      "Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %reduce-window.28 = f32[2,1,64,64,64]{4,3,2,1,0} reduce-window(f32[2,1,128,128,64]{4,3,2,1,0} %constant.20437, f32[] %constant.2652), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x1_1x1_1x0_0}, to_apply=%region_8.31595, metadata={op_name=\"jit(scan)/jit(main)/while/body/jit(get_gradients)/cond/branch_1_fun/vmap(jvp(jvp(jit(loss))))/vmap(eqx.nn.MaxPool2d)/eqx.nn.Pool/reduce_window_max[window_dimensions=(1, 1, 3, 3, 1) window_strides=(1, 1, 2, 2, 1) padding=((0, 0), (0, 0), (1, 1), (1, 1), (0, 0)) base_dilation=(1, 1, 1, 1, 1) window_dilation=(1, 1, 1, 1, 1)]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/resnet.py\" source_line=357}\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n"
     ]
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "final_params, final_beta, final_states, loss_lists, _ = train_outer(\n",
    "    n_steps_outer, n_steps_inner, init_params, init_beta, static, init_states, train_dataloader, val_dataloader, opt_state_outer, opt_state_inner, key, print_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd0210-005d-4184-8453-833a5c9cc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_lists[0], label=\"elbo\")\n",
    "plt.plot(loss_lists[1], label=\"rec_term\")\n",
    "plt.plot(loss_lists[2], label=\"kld\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1caf0b9-fb30-4f98-81d4-8cee30886fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import LivestockTestDataset\n",
    "test_dataset = LivestockTestDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1024,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14631ec-9781-46d2-be62-697c939dd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = next(iter(test_dataloader))\n",
    "x_test =  x_test[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356df63e-7e99-4f83-93dd-645af7cdb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "(_, aux_loss) = loss(final_params, static, final_states, x_test, subkey, train=False, beta=beta)\n",
    "x_rec_test = aux_loss[0]\n",
    "\n",
    "vae_mu = aux_loss[-1]\n",
    "mad = jnp.mean(jnp.abs(vae_mu - jnp.mean(vae_mu, axis=(1), keepdims=True)), axis=(1)) # mean on latent dims for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bea16-caf1-41b4-9a63-6e857909165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 4)\n",
    "axes[0, 0].imshow(jnp.moveaxis(x_test[1],0, 2))\n",
    "axes[1, 0].imshow(jnp.moveaxis(x_rec_test[1], 0, 2))\n",
    "axes[0, 1].imshow(mad[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5661b1-99f2-4a88-893b-7bc21102d8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
