{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0081623-ddf7-4eea-9904-1237d1385270",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5515a157-74b4-423c-9910-643c6ef5898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import jax\n",
    "from jax import jit, vmap\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from math import prod\n",
    "from jax.experimental import jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5c520e-e429-453d-b493-0e01d7c4bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"0.95\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b108b2a0-1b13-4611-ae2e-19ad8c80f8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c277e07d-1e90-4118-bc91-1dca1cf02b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ab576d-3fc4-4f49-a779-8cb123209ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 14:06:17.741838: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9197164e-9ae0-4615-8582-9b93f5c06ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0d50ae-425f-4f88-b4f9-0520b6ef759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae_jax import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ac78f0-2536-41d9-af3e-bad8f01055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "batch_size = 2\n",
    "latent_img_size = 32\n",
    "z_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb27bf5-75c3-4fe7-a89f-8943e2cc6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, vae_states = eqx.nn.make_with_state(VAE)(img_size, latent_img_size, z_dim, key)\n",
    "init_vae_params, vae_static = eqx.partition(vae, eqx.is_inexact_array)\n",
    "# x = jnp.zeros((1, 3, 256, 256)) # we must have a batch_dim, the model can only be applied afetr vmap because of BN layer\n",
    "# batch_model = jax.vmap(model, in_axes=(0, None, None, None), out_axes=(0, None), axis_name=\"batch\")\n",
    "# x, state = batch_model(x, state, key, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b136a3-1b10-4b5f-a358-87477ffcf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_train_dataloader, get_test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49666ec-6a84-4ad1-9bda-87d1a0eca25d",
   "metadata": {},
   "source": [
    "**Note:** if we were doing this gradient based search of beta without using a validation dataset for the outer loss, should we expect to have a beta estimated to be 0 as it would be optimal to overfit and kill the regularization term right ? -> this seems to be the case emprically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2be38c-7611-4f9b-a4b7-d6f311bf79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import LivestockTrainDataset\n",
    "train_dataset = LivestockTrainDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1500,\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e95dca11-5b50-4318-b670-e7ac3cf852b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LivestockTrainDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1500,\n",
    "    offset_idx=train_dataset.__len__()\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aafa604-c820-494f-8233-ff958c0b6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(2, 6))\n",
    "def loss(params, beta, static, states, x, key, train=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        XXX\n",
    "    static\n",
    "        XXX\n",
    "    states\n",
    "        XXX\n",
    "    x\n",
    "        A batch of inputs\n",
    "    key\n",
    "        A JAX PRNGKey\n",
    "    \"\"\"\n",
    "    vae = eqx.combine(params, static)\n",
    "    if train:\n",
    "        # make sure we are in training mode\n",
    "        vae = eqx.nn.inference_mode(vae, value=False)\n",
    "    else:\n",
    "        vae = eqx.nn.inference_mode(vae)\n",
    "    batched_vae = vmap(vae, in_axes=(0, None, None, None), out_axes=(0,  None, 0, 0), axis_name=\"batch\")\n",
    "\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "\n",
    "    x_rec, states, mu, logvar = batched_vae(x, states, key, train)\n",
    "    batched_elbo = vmap(vae.elbo, in_axes=(0, 0, 0, 0, None), out_axes=(0, 0, 0))\n",
    "\n",
    "    elbo, rec_term, kld = batched_elbo(x_rec, x, mu, logvar, beta)\n",
    "\n",
    "    elbo = jnp.mean(elbo) # avg over the batches\n",
    "    rec_term = jnp.mean(rec_term)\n",
    "    kld = jnp.mean(kld)\n",
    "\n",
    "    x_rec = VAE.mean_from_lambda(x_rec)\n",
    "\n",
    "    # elbo = jnp.array(0.)\n",
    "    # rec_term = jnp.array(0.)\n",
    "    # kld = jnp.array(0.)\n",
    "    # x_rec = jnp.array(0.)\n",
    "    # logvar = jnp.array(0.)\n",
    "    # mu = jnp.array(0.)\n",
    "\n",
    "    return -elbo, (x_rec, rec_term, kld, states, mu, logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fbc4f-f9d6-44f7-a5ce-d485dd10fa21",
   "metadata": {},
   "source": [
    "Test the loss on one mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958301ff-6d72-41e2-a7cc-3403972b11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = next(iter(train_dataloader))\n",
    "#loss_value, (_, _, _, vae_states, _, _) = loss(init_vae_params, 1., vae_static, vae_states, mini_batch[0].numpy(), key, train=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e6587ff-424f-4877-8405-318f67c1212d",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bfb534d-0408-4a26-8178-5754202e4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "init_params = init_vae_params\n",
    "init_states = vae_states\n",
    "static = vae_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6cdcf9e-c250-4d95-8e81-ce1316cdb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = eqx.combine(init_params, static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af1394fd-26b4-480b-86b5-3d47e08001d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "n_steps_inner = 30\n",
    "n_steps_outer = 100\n",
    "\n",
    "lr_inner = 1e-3\n",
    "lr_outer = 1e-2\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "init_beta = jnp.array(0.1)\n",
    "\n",
    "optimizer_inner = optax.adam(lr_inner)\n",
    "opt_state_inner = optimizer_inner.init(init_params)\n",
    "optimizer_outer = optax.adam(lr_outer)\n",
    "opt_state_outer = optimizer_outer.init(init_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "548bb682-1bad-4235-a474-6302c0f67cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_inner(params, beta, static, states, train_data, opt_state, key, print_every):\n",
    "    \"\"\"\n",
    "    must not necesarily be jittable since we a priori only need to get grad\n",
    "    but we provide a jittable train_inner for speed\n",
    "    \"\"\"\n",
    "\n",
    "    def make_step(carry, x):\n",
    "        params, beta, states, opt_state, key = carry\n",
    "        key, subkey = jax.random.split(key)\n",
    "        (minus_elbo, aux_loss), grads = jax.value_and_grad(loss, argnums=0, has_aux=True)(params, beta, static, states, x, subkey, train=True)\n",
    "        _, rec_term, kld, states, _, _ = aux_loss\n",
    "        updates, opt_state = optimizer_inner.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return (params, beta, states, opt_state, key), jnp.array([minus_elbo, rec_term, kld])\n",
    "\n",
    "    carry_init = (\n",
    "        params, beta, states, opt_state, key\n",
    "    )\n",
    "    carry, losses = jax.lax.scan(\n",
    "        make_step,\n",
    "        carry_init,\n",
    "        train_data\n",
    "    )\n",
    "    params = carry[0]\n",
    "    states = carry[2]\n",
    "    opt_state = carry[3]\n",
    "    return params, beta, states, (losses[:, 0], losses[:, 1], losses[:, 2]), opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f8c8a4f-a684-4ddd-9267-2ec00fed519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.custom_jvp, nondiff_argnums=(0, 2, 3, 4, 5, 6)) # we will not differentiate wrt something else than beta\n",
    "def train_inner_(params, beta, static, states, inner_train_data, opt_state_inner, key):\n",
    "    return train_inner(params, beta, static, states, inner_train_data, opt_state_inner, key, print_every)\n",
    "    \n",
    "@train_inner_.defjvp\n",
    "def train_inner_jvp(params0, static, states, x_data, opt_state_inner, key, primals, tangents):\n",
    "    \"\"\"\n",
    "    Note that nondiff arg placed at the start of the signature of the corresponding JVP rule\n",
    "    \"\"\"\n",
    "    print(\"here\")\n",
    "    beta0, = primals\n",
    "    v, = tangents\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # # compute x_0(beta_0) (for a given beta_0). The gives the couple (x0, beta_0) around which we are authorized to use th IFT formula\n",
    "    # params_0_beta, _, _, (_, _, _), _ = train_inner_(\n",
    "    #     params, beta, static, states, inner_train_data, opt_state_inner, subkey\n",
    "    # )\n",
    "    # subkeys not considered below since train=False\n",
    "    Ax = lambda x:-jax.jacfwd(lambda params_:jax.grad(loss, argnums=0, has_aux=True)(params_, beta0, static, states, x_data, subkey, train=False)[0])(params0) @ x\n",
    "    b = jax.jacfwd(lambda beta_:jax.grad(loss, argnums=0, has_aux=True)(params0, beta_, static, states, x_data, subkey, train=False)[0])(beta0) # second diff wrt lambda\n",
    "    #b = jax.jacfwd(lambda beta_:jax.grad(loss, argnums=0, has_aux=True)(params0, beta_, static, states, x_data, subkey, train=False))(b'0')# second diff wrt lambda\n",
    "\n",
    "    return params0, jax.scipy.sparse.linalg.cg(Ax, b) * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d1e8f20b-fb99-4002-9d1d-6b76b55ec371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_outer(n_steps_outer, n_steps_inner, params, beta, static, states, train_loader, val_loader, opt_state_outer, opt_state_inner, key, print_every):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def infinite_val_loader():\n",
    "        while True:\n",
    "            yield from val_loader\n",
    "\n",
    "    def make_step(outer_val_batch, params, beta0, states, inner_train_data, opt_state_outer, opt_state_inner, key):\n",
    "        key, subkey = jax.random.split(key)\n",
    "\n",
    "        # this call to train_inner_ gives a x_0, theta_0 (the resulting couple (params, beta)\n",
    "        # this couple is a root of F(=dELBO(params, beta)/dparams) (for a fixed theta_0 (beta)),\n",
    "        # using a SGD that starts at a x_init (params)\n",
    "        params0, _, states, (elbo_list, rec_term_list, kld_list), opt_state_inner = train_inner_(\n",
    "            params, beta0, static, states, inner_train_data, opt_state_inner, subkey\n",
    "        )\n",
    "        # ... around this x_0, theta_0 we know that we have the right to compute dx*(theta)/dtheta and we have done so\n",
    "        # in the jvp defined in the previous cells\n",
    "\n",
    "        # key, subkey = jax.random.split(key)\n",
    "        # grads_inner = jax.jacfwd(train_inner_, argnums=1)(params, beta, static, states, x, opt_state_inner, subkey)\n",
    "\n",
    "\n",
    "        # Reconstruct the gradient here ?\n",
    "        # Ax = lambda x:-(jax.jacfwd(\n",
    "        #         lambda params_:jax.grad(\n",
    "        #             loss, argnums=0, has_aux=True\n",
    "        #         )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        #     )(params0) @ x)\n",
    "        # def mul_(x, y):\n",
    "        #     print(x)\n",
    "        #     print(y)\n",
    "        #     return x @ y\n",
    "        # Ax  = lambda x:jax.tree.map(mul_, #lambda x_, y:x_ @ y,\n",
    "        #         x,\n",
    "        #         jax.jacfwd(\n",
    "        #             lambda params_:jax.grad(\n",
    "        #                 loss, argnums=0, has_aux=True\n",
    "        #             )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        #         )(params0),\n",
    "        #         is_leaf=lambda x_: eqx.is_inexact_array(x_))\n",
    "        # Ax = \n",
    "\n",
    "        def Ax(x):\n",
    "            # trying jax.jet with https://github.com/google/jax/discussions/9598\n",
    "\n",
    "            loss_ = lambda p: loss(p, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "            def loss_wrapper(*true_params, tree):\n",
    "                \"\"\"To be able to differentiate the certain elements of the pytree\n",
    "                https://github.com/google/jax/discussions/12765\n",
    "\n",
    "                true_params is a list of the arrays with respect to which we want to differentiate inside the eqx.Module\n",
    "                defined by params0\n",
    "\n",
    "                Note that all that goes after *args are keyword only arguments\n",
    "                \"\"\"\n",
    "                p = jax.tree.unflatten(tree, true_params)\n",
    "                # p = jax.tree_util.tree_map_with_path(\n",
    "                #     lambda kp, p: dict_path_true_params[kp], # kp will nec be in dict_path_true_params because of the following is_leaf function\n",
    "                #     params0,\n",
    "                #     is_leaf=eqx.is_inexact_array\n",
    "                # )\n",
    "                return loss_(p)\n",
    "\n",
    "            # params0_fl = jax.tree.map(\n",
    "            #     lambda x_: x_.flatten(),\n",
    "            #     params0,\n",
    "            #     is_leaf=eqx.is_inexact_array\n",
    "            # )\n",
    "            true_params, tree = jax.tree_util.tree_flatten(params0) # in the children non static content (param_fl) only the jnp arrays go\n",
    "            # all None and integer constant go to tree so a tree_flatten suffices to get the true params\n",
    "            # we really optimize upon\n",
    "            \n",
    "\n",
    "            # true_params_identity_filled = jax.tree.map(\n",
    "            #     lambda x_:jnp.eye(x_.shape[0]),\n",
    "            #     true_params,\n",
    "            # )\n",
    "            # print(true_params)\n",
    "            # jet.jet(loss_, (true_params,), ((true_params_identity_filled,)))[1][0]\n",
    "            # fs\n",
    "            # # jet.jet(fun, (x,), ((v, jnp.zeros_like(x)),))[1][1]\n",
    "            # #     return jnp.sum(hvv(jnp.eye(x.shape[0], dtype=x.dtype)))\n",
    "            \n",
    "            # jet.jet(loss_, (params0,), )[1][1]\n",
    "\n",
    "            # # dL_dtheta = lambda params_:jax.grad(\n",
    "            # #         loss, argnums=0, has_aux=True\n",
    "            # #     )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "            \n",
    "            \n",
    "            pytree_grad = [\n",
    "                jax.jacfwd(\n",
    "                    lambda *params_:jax.grad(loss_wrapper, argnums=i)(*params_, tree=tree),\n",
    "                    argnums=i\n",
    "                )(*true_params)\n",
    "                for i in range(len(true_params))\n",
    "            ]\n",
    "            fs\n",
    "            pytree_grad = jax.hessian(\n",
    "                loss, argnums=0, has_aux=True\n",
    "            )(params0, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "\n",
    "            # for each inexact field (x_) of the VAE params we only compute the @ (and let a non None value)\n",
    "            # with the array (y_) at the same position of the nested VAE params located (at the x_ inexact field of the outer VAE params)\n",
    "            # this can be seen as retrieving the diagonal of the Hessian matrix\n",
    "            nested_vaes = jax.tree_util.tree_map_with_path(\n",
    "                lambda key_path_x_, x_, y: jax.tree_util.tree_map_with_path(\n",
    "                    lambda key_path_y_, y_: (y_.reshape((prod(x_.shape), prod(x_.shape))) @ x_.flatten()).reshape(x_.shape) if key_path_x_ == key_path_y_ else None,\n",
    "                    y,\n",
    "                    is_leaf=eqx.is_inexact_array\n",
    "                ),\n",
    "                x,\n",
    "                pytree_grad,\n",
    "                is_leaf=eqx.is_inexact_array\n",
    "            )\n",
    "                \n",
    "            outer_vae = jax.tree_util.tree_map_with_path(\n",
    "                lambda key_path_x_, x_, y: jax.tree.leaves(y, is_leaf=eqx.is_inexact_array)[0],\n",
    "                x,\n",
    "                nested_vaes,\n",
    "                is_leaf=eqx.is_inexact_array\n",
    "            )\n",
    "            #outer_vae = jax.tree.leaves(nested_vaes, is_leaf=eqx.is_inexact_array)\n",
    "            #print(outer_vae)\n",
    "                \n",
    "            #print(outer_vae)\n",
    "            return outer_vae\n",
    "                \n",
    "            \n",
    "        b = jax.jacfwd(lambda beta_:jax.grad(loss, argnums=0, has_aux=True)(params0, beta_, static, states, outer_val_batch, subkey, train=False)[0])(beta0)\n",
    "        #b = jax.tree.leaves(b, is_leaf=eqx.is_inexact_array)\n",
    "        #print(\"B\", b)\n",
    "        grads_inner = jax.scipy.sparse.linalg.cg(Ax, b)\n",
    "        print(grads_inner.shape)\n",
    "\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # Now we want to compute the ELBO for the outer loss and take gradient wrt to theta here.\n",
    "        # We have defined a rule to backpropagate through beta i.e.\n",
    "        # through train_inner (dx*(theta)/dtheta) in the previous cells\n",
    "        (minus_elbo, aux_loss), grads_outer = jax.value_and_grad(loss, argnums=1, has_aux=True)(params, beta, static, states, outer_val_batch, subkey, train=False)\n",
    "        x_rec, rec_term, kld, _, _, _ = aux_loss\n",
    "        print(grads_outer.shape)\n",
    "        grads = grads_outer @ grads_inner\n",
    "        print(grads.shape)\n",
    "        updates, opt_state_outer = optimizer_outer.update(grads, opt_state_outer, beta)\n",
    "        beta = optax.apply_updates(beta, updates)\n",
    "        return params, beta, states, opt_state_outer, opt_state_inner, key, x_rec, (elbo_list, rec_term_list, kld_list)\n",
    "\n",
    "    elbo_list = []\n",
    "    rec_term_list = []\n",
    "    kld_list = []\n",
    "    beta_list = []\n",
    "\n",
    "\n",
    "    for step, (x, _) in zip(range(n_steps_outer), infinite_val_loader()): \n",
    "        x = x.numpy()\n",
    "        inner_train_data = jnp.asarray(\n",
    "            list(map(lambda x:x[0].numpy(), list(itertools.islice(train_loader, n_steps_inner)))) # get the next n_steps_inner elements from train_loader\n",
    "        )\n",
    "        params, beta, states, opt_state_outer, opt_state_inner, key, x_rec, losses = make_step(\n",
    "            x, params, beta, states, inner_train_data, opt_state_outer, opt_state_inner, key\n",
    "        )\n",
    "        elbo_list.extend(-losses[0])\n",
    "        rec_term_list.extend(losses[1])\n",
    "        kld_list.extend(losses[2])\n",
    "        # elbo_list.append(-losses[0])\n",
    "        # rec_term_list.append(losses[1])\n",
    "        # kld_list.append(losses[2])\n",
    "        beta_list.append(jnp.full((n_steps_inner,), beta))\n",
    "        \n",
    "        if (step % print_every) == 0 or (step == n_steps - 1):\n",
    "            print(\n",
    "                f\"{step=}, elbo_loss={elbo_list[-1]}, rec_term={rec_term_list[-1]}, kld_term={kld_list[-1]}, beta={beta_list[-1][0]}\"\n",
    "            )\n",
    "            \n",
    "    return params, beta, states, (elbo_list, rec_term_list, kld_list), (opt_state_outer, opt_state_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "adcfb805-5c54-42fb-9b83-4e7845ff23de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1.0, None), (2.0, None), (3.0, None, 4.0))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree.map(lambda x, y: jax.tree.map(lambda y_:x+y_, y, is_leaf=lambda x:(isinstance(x, float))),\n",
    "             (0., 0., 0.),\n",
    "             ((1., None), (2., None), (3., None, 4)),\n",
    "             is_leaf=lambda x:(isinstance(x, float)))# and isinstance(y, tuple)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "540a810f-bd72-4da6-b317-5f9c555c9d10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m----> 2\u001b[0m final_params, final_beta, final_states, loss_lists, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_outer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_outer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_beta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state_outer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 163\u001b[0m, in \u001b[0;36mtrain_outer\u001b[0;34m(n_steps_outer, n_steps_inner, params, beta, static, states, train_loader, val_loader, opt_state_outer, opt_state_inner, key, print_every)\u001b[0m\n\u001b[1;32m    159\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    160\u001b[0m inner_train_data \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(train_loader, n_steps_inner)))) \u001b[38;5;66;03m# get the next n_steps_inner elements from train_loader\u001b[39;00m\n\u001b[1;32m    162\u001b[0m )\n\u001b[0;32m--> 163\u001b[0m params, beta, states, opt_state_outer, opt_state_inner, key, x_rec, losses \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state_outer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m elbo_list\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;241m-\u001b[39mlosses[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m rec_term_list\u001b[38;5;241m.\u001b[39mextend(losses[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[96], line 135\u001b[0m, in \u001b[0;36mtrain_outer.<locals>.make_step\u001b[0;34m(outer_val_batch, params, beta0, states, inner_train_data, opt_state_outer, opt_state_inner, key)\u001b[0m\n\u001b[1;32m    132\u001b[0m b \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjacfwd(\u001b[38;5;28;01mlambda\u001b[39;00m beta_:jax\u001b[38;5;241m.\u001b[39mgrad(loss, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(params0, beta_, static, states, outer_val_batch, subkey, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])(beta0)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m#b = jax.tree.leaves(b, is_leaf=eqx.is_inexact_array)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#print(\"B\", b)\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m grads_inner \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(grads_inner\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    139\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/vae_grf/lib/python3.11/site-packages/jax/_src/scipy/sparse/linalg.py:290\u001b[0m, in \u001b[0;36mcg\u001b[0;34m(A, b, x0, tol, atol, maxiter, M)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcg\u001b[39m(A, b, x0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, maxiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, M\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Use Conjugate Gradient iteration to solve ``Ax = b``.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  The numerics of JAX's ``cg`` should exact match SciPy's ``cg`` (up to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m  jax.lax.custom_linear_solve\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cg_solve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_symmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vae_grf/lib/python3.11/site-packages/jax/_src/scipy/sparse/linalg.py:230\u001b[0m, in \u001b[0;36m_isolve\u001b[0;34m(_isolve_solve, A, b, x0, tol, atol, maxiter, M, check_symmetric)\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mcomplexfloating)\n\u001b[1;32m    228\u001b[0m symmetric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mmap\u001b[39m(real_valued, tree_leaves(b))) \\\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m check_symmetric \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_linear_solve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43misolve_solve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose_solve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43misolve_solve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43msymmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, info\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[96], line 100\u001b[0m, in \u001b[0;36mtrain_outer.<locals>.make_step.<locals>.Ax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# all None and integer constant go to tree so a tree_flatten suffices to get the true params\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# we really optimize upon\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# #         loss, argnums=0, has_aux=True\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# #     )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\u001b[39;00m\n\u001b[1;32m     93\u001b[0m pytree_grad \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     94\u001b[0m     jax\u001b[38;5;241m.\u001b[39mjacfwd(\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39mparams_:jax\u001b[38;5;241m.\u001b[39mgrad(loss_wrapper, argnums\u001b[38;5;241m=\u001b[39mi)(\u001b[38;5;241m*\u001b[39mparams_, tree\u001b[38;5;241m=\u001b[39mtree),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(true_params))\n\u001b[1;32m     99\u001b[0m ]\n\u001b[0;32m--> 100\u001b[0m \u001b[43mfs\u001b[49m\n\u001b[1;32m    101\u001b[0m pytree_grad \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mhessian(\n\u001b[1;32m    102\u001b[0m     loss, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    103\u001b[0m )(params0, beta0, static, states, outer_val_batch, subkey, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# for each inexact field (x_) of the VAE params we only compute the @ (and let a non None value)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# with the array (y_) at the same position of the nested VAE params located (at the x_ inexact field of the outer VAE params)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# this can be seen as retrieving the diagonal of the Hessian matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fs' is not defined"
     ]
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "final_params, final_beta, final_states, loss_lists, _ = train_outer(\n",
    "    n_steps_outer, n_steps_inner, init_params, init_beta, static, init_states, train_dataloader, val_dataloader, opt_state_outer, opt_state_inner, key, print_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd0210-005d-4184-8453-833a5c9cc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_lists[0], label=\"elbo\")\n",
    "plt.plot(loss_lists[1], label=\"rec_term\")\n",
    "plt.plot(loss_lists[2], label=\"kld\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1caf0b9-fb30-4f98-81d4-8cee30886fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import LivestockTestDataset\n",
    "test_dataset = LivestockTestDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1024,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14631ec-9781-46d2-be62-697c939dd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = next(iter(test_dataloader))\n",
    "x_test =  x_test[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356df63e-7e99-4f83-93dd-645af7cdb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "(_, aux_loss) = loss(final_params, static, final_states, x_test, subkey, train=False, beta=beta)\n",
    "x_rec_test = aux_loss[0]\n",
    "\n",
    "vae_mu = aux_loss[-1]\n",
    "mad = jnp.mean(jnp.abs(vae_mu - jnp.mean(vae_mu, axis=(1), keepdims=True)), axis=(1)) # mean on latent dims for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bea16-caf1-41b4-9a63-6e857909165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 4)\n",
    "axes[0, 0].imshow(jnp.moveaxis(x_test[1],0, 2))\n",
    "axes[1, 0].imshow(jnp.moveaxis(x_rec_test[1], 0, 2))\n",
    "axes[0, 1].imshow(mad[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5661b1-99f2-4a88-893b-7bc21102d8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
