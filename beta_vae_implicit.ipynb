{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0081623-ddf7-4eea-9904-1237d1385270",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5515a157-74b4-423c-9910-643c6ef5898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import jax\n",
    "from jax import jit, vmap\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from math import prod\n",
    "from jax.experimental import jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5c520e-e429-453d-b493-0e01d7c4bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"0.95\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b108b2a0-1b13-4611-ae2e-19ad8c80f8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c277e07d-1e90-4118-bc91-1dca1cf02b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ab576d-3fc4-4f49-a779-8cb123209ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 09:29:27.581605: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9197164e-9ae0-4615-8582-9b93f5c06ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0d50ae-425f-4f88-b4f9-0520b6ef759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae_jax import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ac78f0-2536-41d9-af3e-bad8f01055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "batch_size = 2\n",
    "latent_img_size = 32\n",
    "z_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb27bf5-75c3-4fe7-a89f-8943e2cc6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, vae_states = eqx.nn.make_with_state(VAE)(img_size, latent_img_size, z_dim, key)\n",
    "init_vae_params, vae_static = eqx.partition(vae, eqx.is_inexact_array)\n",
    "# x = jnp.zeros((1, 3, 256, 256)) # we must have a batch_dim, the model can only be applied afetr vmap because of BN layer\n",
    "# batch_model = jax.vmap(model, in_axes=(0, None, None, None), out_axes=(0, None), axis_name=\"batch\")\n",
    "# x, state = batch_model(x, state, key, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b136a3-1b10-4b5f-a358-87477ffcf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_train_dataloader, get_test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49666ec-6a84-4ad1-9bda-87d1a0eca25d",
   "metadata": {},
   "source": [
    "**Note:** if we were doing this gradient based search of beta without using a validation dataset for the outer loss, should we expect to have a beta estimated to be 0 as it would be optimal to overfit and kill the regularization term right ? -> this seems to be the case emprically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2be38c-7611-4f9b-a4b7-d6f311bf79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import LivestockTrainDataset\n",
    "train_dataset = LivestockTrainDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1500,\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e95dca11-5b50-4318-b670-e7ac3cf852b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LivestockTrainDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1500,\n",
    "    offset_idx=train_dataset.__len__()\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aafa604-c820-494f-8233-ff958c0b6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(2, 6))\n",
    "def loss(params, beta, static, states, x, key, train=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        XXX\n",
    "    static\n",
    "        XXX\n",
    "    states\n",
    "        XXX\n",
    "    x\n",
    "        A batch of inputs\n",
    "    key\n",
    "        A JAX PRNGKey\n",
    "    \"\"\"\n",
    "    vae = eqx.combine(params, static)\n",
    "    if train:\n",
    "        # make sure we are in training mode\n",
    "        vae = eqx.nn.inference_mode(vae, value=False)\n",
    "    else:\n",
    "        vae = eqx.nn.inference_mode(vae)\n",
    "    batched_vae = vmap(vae, in_axes=(0, None, None, None), out_axes=(0,  None, 0, 0), axis_name=\"batch\")\n",
    "\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "\n",
    "    x_rec, states, mu, logvar = batched_vae(x, states, key, train)\n",
    "    batched_elbo = vmap(vae.elbo, in_axes=(0, 0, 0, 0, None), out_axes=(0, 0, 0))\n",
    "\n",
    "    elbo, rec_term, kld = batched_elbo(x_rec, x, mu, logvar, beta)\n",
    "\n",
    "    elbo = jnp.mean(elbo) # avg over the batches\n",
    "    rec_term = jnp.mean(rec_term)\n",
    "    kld = jnp.mean(kld)\n",
    "\n",
    "    x_rec = VAE.mean_from_lambda(x_rec)\n",
    "\n",
    "    # elbo = jnp.array(0.)\n",
    "    # rec_term = jnp.array(0.)\n",
    "    # kld = jnp.array(0.)\n",
    "    # x_rec = jnp.array(0.)\n",
    "    # logvar = jnp.array(0.)\n",
    "    # mu = jnp.array(0.)\n",
    "\n",
    "    return -elbo, (x_rec, rec_term, kld, states, mu, logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fbc4f-f9d6-44f7-a5ce-d485dd10fa21",
   "metadata": {},
   "source": [
    "Test the loss on one mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958301ff-6d72-41e2-a7cc-3403972b11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = next(iter(train_dataloader))\n",
    "#loss_value, (_, _, _, vae_states, _, _) = loss(init_vae_params, 1., vae_static, vae_states, mini_batch[0].numpy(), key, train=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e6587ff-424f-4877-8405-318f67c1212d",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bfb534d-0408-4a26-8178-5754202e4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "init_params = init_vae_params\n",
    "init_states = vae_states\n",
    "static = vae_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6cdcf9e-c250-4d95-8e81-ce1316cdb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = eqx.combine(init_params, static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af1394fd-26b4-480b-86b5-3d47e08001d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "n_steps_inner = 30\n",
    "n_steps_outer = 100\n",
    "\n",
    "lr_inner = 1e-3\n",
    "lr_outer = 1e-2\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "init_beta = jnp.array(0.1)\n",
    "\n",
    "optimizer_inner = optax.adam(lr_inner)\n",
    "opt_state_inner = optimizer_inner.init(init_params)\n",
    "optimizer_outer = optax.adam(lr_outer)\n",
    "opt_state_outer = optimizer_outer.init(init_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "548bb682-1bad-4235-a474-6302c0f67cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_inner(params, beta, static, states, train_data, opt_state, key, print_every):\n",
    "    \"\"\"\n",
    "    must not necesarily be jittable since we a priori only need to get grad\n",
    "    but we provide a jittable train_inner for speed\n",
    "    \"\"\"\n",
    "\n",
    "    def make_step(carry, x):\n",
    "        params, beta, states, opt_state, key = carry\n",
    "        key, subkey = jax.random.split(key)\n",
    "        (minus_elbo, aux_loss), grads = jax.value_and_grad(loss, argnums=0, has_aux=True)(params, beta, static, states, x, subkey, train=True)\n",
    "        _, rec_term, kld, states, _, _ = aux_loss\n",
    "        updates, opt_state = optimizer_inner.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return (params, beta, states, opt_state, key), jnp.array([minus_elbo, rec_term, kld])\n",
    "\n",
    "    carry_init = (\n",
    "        params, beta, states, opt_state, key\n",
    "    )\n",
    "    carry, losses = jax.lax.scan(\n",
    "        make_step,\n",
    "        carry_init,\n",
    "        train_data\n",
    "    )\n",
    "    params = carry[0]\n",
    "    states = carry[2]\n",
    "    opt_state = carry[3]\n",
    "    return params, beta, states, (losses[:, 0], losses[:, 1], losses[:, 2]), opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f8c8a4f-a684-4ddd-9267-2ec00fed519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.custom_jvp, nondiff_argnums=(0, 2, 3, 4, 5, 6)) # we will not differentiate wrt something else than beta\n",
    "def train_inner_(params, beta, static, states, inner_train_data, opt_state_inner, key):\n",
    "    return train_inner(params, beta, static, states, inner_train_data, opt_state_inner, key, print_every)\n",
    "    \n",
    "@train_inner_.defjvp\n",
    "def train_inner_jvp(params0, static, states, x_data, opt_state_inner, key, primals, tangents):\n",
    "    \"\"\"\n",
    "    Note that nondiff arg placed at the start of the signature of the corresponding JVP rule\n",
    "    \"\"\"\n",
    "    print(\"here\")\n",
    "    beta0, = primals\n",
    "    v, = tangents\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # # compute x_0(beta_0) (for a given beta_0). The gives the couple (x0, beta_0) around which we are authorized to use th IFT formula\n",
    "    # params_0_beta, _, _, (_, _, _), _ = train_inner_(\n",
    "    #     params, beta, static, states, inner_train_data, opt_state_inner, subkey\n",
    "    # )\n",
    "    # subkeys not considered below since train=False\n",
    "    Ax = lambda x:-jax.jacfwd(lambda params_:jax.grad(loss, argnums=0, has_aux=True)(params_, beta0, static, states, x_data, subkey, train=False)[0])(params0) @ x\n",
    "    b = jax.jacfwd(lambda beta_:jax.grad(loss, argnums=0, has_aux=True)(params0, beta_, static, states, x_data, subkey, train=False)[0])(beta0) # second diff wrt lambda\n",
    "    #b = jax.jacfwd(lambda beta_:jax.grad(loss, argnums=0, has_aux=True)(params0, beta_, static, states, x_data, subkey, train=False))(b'0')# second diff wrt lambda\n",
    "\n",
    "    return params0, jax.scipy.sparse.linalg.cg(Ax, b) * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d1e8f20b-fb99-4002-9d1d-6b76b55ec371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_outer(n_steps_outer, n_steps_inner, params, beta, static, states, train_loader, val_loader, opt_state_outer, opt_state_inner, key, print_every):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def infinite_val_loader():\n",
    "        while True:\n",
    "            yield from val_loader\n",
    "\n",
    "    def make_step(outer_val_batch, params, beta0, states, inner_train_data, opt_state_outer, opt_state_inner, key):\n",
    "        key, subkey = jax.random.split(key)\n",
    "\n",
    "        # this call to train_inner_ gives a x_0, theta_0 (the resulting couple (params, beta)\n",
    "        # this couple is a root of F(=dELBO(params, beta)/dparams) (for a fixed theta_0 (beta)),\n",
    "        # using a SGD that starts at a x_init (params)\n",
    "        params0, _, states, (elbo_list, rec_term_list, kld_list), opt_state_inner = train_inner_(\n",
    "            params, beta0, static, states, inner_train_data, opt_state_inner, subkey\n",
    "        )\n",
    "        # ... around this x_0, theta_0 we know that we have the right to compute dx*(theta)/dtheta and we have done so\n",
    "        # in the jvp defined in the previous cells\n",
    "\n",
    "        # key, subkey = jax.random.split(key)\n",
    "        # grads_inner = jax.jacfwd(train_inner_, argnums=1)(params, beta, static, states, x, opt_state_inner, subkey)\n",
    "\n",
    "\n",
    "        # Reconstruct the gradient here ?\n",
    "        # Ax = lambda x:-(jax.jacfwd(\n",
    "        #         lambda params_:jax.grad(\n",
    "        #             loss, argnums=0, has_aux=True\n",
    "        #         )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        #     )(params0) @ x)\n",
    "        # def mul_(x, y):\n",
    "        #     print(x)\n",
    "        #     print(y)\n",
    "        #     return x @ y\n",
    "        # Ax  = lambda x:jax.tree.map(mul_, #lambda x_, y:x_ @ y,\n",
    "        #         x,\n",
    "        #         jax.jacfwd(\n",
    "        #             lambda params_:jax.grad(\n",
    "        #                 loss, argnums=0, has_aux=True\n",
    "        #             )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        #         )(params0),\n",
    "        #         is_leaf=lambda x_: eqx.is_inexact_array(x_))\n",
    "        # Ax = \n",
    "\n",
    "        # def Ax(x):\n",
    "        #     # trying jax.jet with https://github.com/google/jax/discussions/9598\n",
    "\n",
    "        #     loss_ = lambda p: loss(p, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        #     def loss_wrapper(*true_params, tree):\n",
    "        #         \"\"\"To be able to differentiate the certain elements of the pytree\n",
    "        #         https://github.com/google/jax/discussions/12765\n",
    "\n",
    "        #         true_params is a list of the arrays with respect to which we want to differentiate inside the eqx.Module\n",
    "        #         defined by params0\n",
    "\n",
    "        #         Note that all that goes after *args are keyword only arguments\n",
    "        #         \"\"\"\n",
    "        #         p = jax.tree.unflatten(tree, true_params)\n",
    "        #         # p = jax.tree_util.tree_map_with_path(\n",
    "        #         #     lambda kp, p: dict_path_true_params[kp], # kp will nec be in dict_path_true_params because of the following is_leaf function\n",
    "        #         #     params0,\n",
    "        #         #     is_leaf=eqx.is_inexact_array\n",
    "        #         # )\n",
    "        #         return loss_(p)\n",
    "\n",
    "        #     # params0_fl = jax.tree.map(\n",
    "        #     #     lambda x_: x_.flatten(),\n",
    "        #     #     params0,\n",
    "        #     #     is_leaf=eqx.is_inexact_array\n",
    "        #     # )\n",
    "        #     true_params, tree = jax.tree_util.tree_flatten(params0) # in the children non static content (param_fl) only the jnp arrays go\n",
    "        #     # all None and integer constant go to tree so a tree_flatten suffices to get the true params\n",
    "        #     # we really optimize upon\n",
    "            \n",
    "\n",
    "        #     # true_params_identity_filled = jax.tree.map(\n",
    "        #     #     lambda x_:jnp.eye(x_.shape[0]),\n",
    "        #     #     true_params,\n",
    "        #     # )\n",
    "        #     # print(true_params)\n",
    "        #     # jet.jet(loss_, (true_params,), ((true_params_identity_filled,)))[1][0]\n",
    "        #     # fs\n",
    "        #     # # jet.jet(fun, (x,), ((v, jnp.zeros_like(x)),))[1][1]\n",
    "        #     # #     return jnp.sum(hvv(jnp.eye(x.shape[0], dtype=x.dtype)))\n",
    "            \n",
    "        #     # jet.jet(loss_, (params0,), )[1][1]\n",
    "\n",
    "        #     # # dL_dtheta = lambda params_:jax.grad(\n",
    "        #     # #         loss, argnums=0, has_aux=True\n",
    "        #     # #     )(params_, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "            \n",
    "            \n",
    "        #     d2L_dtheta2 = [\n",
    "        #         jax.jacfwd(\n",
    "        #             lambda *params_:jax.grad(loss_wrapper, argnums=i)(*params_, tree=tree),\n",
    "        #             argnums=i\n",
    "        #         )(*true_params)\n",
    "        #         for i in range(len(true_params))\n",
    "        #     ]\n",
    "        #     print(\"A\", d2L_dtheta2)\n",
    "        #     d2L_dtheta2_pytree = jax.tree.unflatten(tree, d2L_dtheta2)\n",
    "        #     res = jax.tree.map(\n",
    "        #         lambda x_, y: (y.reshape((prod(x_.shape), prod(x_.shape))) @ x_.flatten()).reshape(x_.shape),\n",
    "        #         x,\n",
    "        #         d2L_dtheta2_pytree,\n",
    "        #         is_leaf=eqx.is_inexact_array\n",
    "        #     )\n",
    "        #     print(\"B\", res)\n",
    "        #     return res\n",
    "            # # for each inexact field (x_) of the VAE params we only compute the @ (and let a non None value)\n",
    "            # # with the array (y_) at the same position of the nested VAE params located (at the x_ inexact field of the outer VAE params)\n",
    "            # # this can be seen as retrieving the diagonal of the Hessian matrix\n",
    "            # nested_vaes = jax.tree_util.tree_map_with_path(\n",
    "            #     lambda key_path_x_, x_, y: jax.tree_util.tree_map_with_path(\n",
    "            #         lambda key_path_y_, y_: (y_.reshape((prod(x_.shape), prod(x_.shape))) @ x_.flatten()).reshape(x_.shape) if key_path_x_ == key_path_y_ else None,\n",
    "            #         y,\n",
    "            #         is_leaf=eqx.is_inexact_array\n",
    "            #     ),\n",
    "            #     x,\n",
    "            #     pytree_grad,\n",
    "            #     is_leaf=eqx.is_inexact_array\n",
    "            # )\n",
    "                \n",
    "            # outer_vae = jax.tree_util.tree_map_with_path(\n",
    "            #     lambda key_path_x_, x_, y: jax.tree.leaves(y, is_leaf=eqx.is_inexact_array)[0],\n",
    "            #     x,\n",
    "            #     nested_vaes,\n",
    "            #     is_leaf=eqx.is_inexact_array\n",
    "            # )\n",
    "            # #outer_vae = jax.tree.leaves(nested_vaes, is_leaf=eqx.is_inexact_array)\n",
    "            # #print(outer_vae)\n",
    "                \n",
    "            # #print(outer_vae)\n",
    "            # return outer_vae\n",
    "                \n",
    "            \n",
    "        #b = jax.jacfwd(lambda beta_:jax.grad(loss, argnums=0, has_aux=True)(params0, beta_, static, states, outer_val_batch, subkey, train=False)[0])(beta0)\n",
    "        \n",
    "        true_params, tree = jax.tree_util.tree_flatten(params0)\n",
    "        true_params_flattened = jax.tree.map(lambda x: x.flatten(), true_params, is_leaf=eqx.is_inexact_array)\n",
    "        nb_params_array = len(true_params)\n",
    "\n",
    "        loss_ = lambda p: loss(p, beta0, static, states, outer_val_batch, subkey, train=False)[0]\n",
    "        def loss_wrapper(*true_params_flattened, tree):\n",
    "            \"\"\"To be able to differentiate the certain elements of the pytree\n",
    "            https://github.com/google/jax/discussions/12765\n",
    "\n",
    "            true_params is a list of the arrays with respect to which we want to differentiate inside the eqx.Module\n",
    "            defined by params0\n",
    "\n",
    "            Note that all that goes after *args are keyword only arguments\n",
    "            \"\"\"\n",
    "            print(true_params[0].shape, list(true_params_flattened)[0].shape)\n",
    "            true_params_unflattened = jax.tree.map(\n",
    "                lambda x, y:x.reshape(y.shape),\n",
    "                list(true_params_flattened),\n",
    "                true_params,\n",
    "                #is_leaf=eqx.is_inexact_array\n",
    "            )\n",
    "            p = jax.tree.unflatten(tree, true_params_unflattened)\n",
    "            \n",
    "            return loss_(p)\n",
    "\n",
    "        # # Note that we cannot have tracer for grad's argnum (https://stackoverflow.com/questions/77913154/vectorizing-power-of-jax-grad)\n",
    "        # @partial(jax.jit, static_argnums=(1,))\n",
    "        # def get_gradients(idx, nb_params_array):\n",
    "        #     \"\"\"\n",
    "        #     Returns the idx gradient among the nb_params_array gradients that are precomputed at trace time\n",
    "        #     \"\"\"\n",
    "        loss_wrapper_2 = lambda p: loss_wrapper(*true_params_flattened[:0], p, *true_params_flattened[0+1:], tree=tree)\n",
    "        \n",
    "        # NOTE fails with JAX ERROR\n",
    "        # print(jet.jet(\n",
    "        #     loss_wrapper_2,\n",
    "        #     (true_params_flattened[0],),\n",
    "        #     ((jnp.ones_like(true_params_flattened[0]), jnp.zeros_like(true_params_flattened[0])),)\n",
    "        # )[1][1])\n",
    "\n",
    "        print(jax.jacfwd(\n",
    "                    lambda p:jax.grad(loss_wrapper_2, argnums=0)(p),\n",
    "                    argnums=0\n",
    "                )(true_params_flattened[0])\n",
    "            )\n",
    "        fs\n",
    "        \n",
    "        d2L_dtheta2_ = []\n",
    "        d2L_dthetadbeta_ = []\n",
    "        for i in range(nb_params_array):\n",
    "            loss_wrapper_2 = lambda p: loss_wrapper(*true_params[:i], p, *true_params[i+1:], tree)(true_params[i])\n",
    "            \n",
    "            print(i)\n",
    "            d2L_dtheta2_.append(\n",
    "                jax.jacfwd(\n",
    "                    lambda *params_:jax.grad(loss_wrapper, argnums=i)(*params_, tree=tree),\n",
    "                    argnums=i\n",
    "                )(*true_params)\n",
    "            )\n",
    "            d2L_dthetadbeta_.append(\n",
    "                jax.jacfwd(lambda beta_:jax.grad(loss_wrapper, argnums=i)(*true_params, tree=tree))(beta0)\n",
    "            )\n",
    "            # return (jax.lax.switch(idx, d2L_dtheta2_, ()), jax.lax.switch(idx, d2L_dthetadbeta_, ()))\n",
    "        \n",
    "        # def get_gradients_wrapper(idx):\n",
    "        #     return get_gradients(idx, nb_params_array)\n",
    "        # get_gradients_wrapper(0)\n",
    "        print(d2L_dtheta2_[0])\n",
    "        fs\n",
    "        def scan_fun(_, i):\n",
    "            d2L_dtheta2_i, d2L_dthetadbeta_i = get_gradients_wrapper(i)\n",
    "            print(d2L_dtheta2_i.shape, d2L_dthetadbeta_i.shape)\n",
    "            def Ax(x):\n",
    "                return (d2L_dtheta2_i.reshape((prod(x.shape), prod(x.shape))) @ x.flatten()).reshape(x.shape)\n",
    "            grads = jax.scipy.sparse.linalg.cg(Ax, d2L_dthetadbeta_i)\n",
    "            return None, grads\n",
    "        grads_inner = []\n",
    "        for i in range(nb_params_array):\n",
    "            d2L_dtheta2_i, d2L_dthetadbeta_i = get_gradients_wrapper(i)\n",
    "            print(d2L_dtheta2_i.shape, d2L_dthetadbeta_i.shape)\n",
    "            def Ax(x):\n",
    "                return (d2L_dtheta2_i.reshape((prod(x.shape), prod(x.shape))) @ x.flatten()).reshape(x.shape)\n",
    "            grads_inner.append(jax.scipy.sparse.linalg.cg(Ax, d2L_dthetadbeta_i))\n",
    "        print(grads_inner)\n",
    "\n",
    "        # _, grads_inner = jax.lax.scan(\n",
    "        #     scan_fun,\n",
    "        #     None,\n",
    "        #     jnp.arange(nb_params_array)\n",
    "        # )\n",
    "        # print(grads_inner)\n",
    "        fs\n",
    "\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # Now we want to compute the ELBO for the outer loss and take gradient wrt to theta here.\n",
    "        # We have defined a rule to backpropagate through beta i.e.\n",
    "        # through train_inner (dx*(theta)/dtheta) in the previous cells\n",
    "        (minus_elbo, aux_loss), grads_outer = jax.value_and_grad(loss, argnums=1, has_aux=True)(params, beta, static, states, outer_val_batch, subkey, train=False)\n",
    "        x_rec, rec_term, kld, _, _, _ = aux_loss\n",
    "        print(grads_outer.shape)\n",
    "        grads = grads_outer @ grads_inner\n",
    "        print(grads.shape)\n",
    "        updates, opt_state_outer = optimizer_outer.update(grads, opt_state_outer, beta)\n",
    "        beta = optax.apply_updates(beta, updates)\n",
    "        return params, beta, states, opt_state_outer, opt_state_inner, key, x_rec, (elbo_list, rec_term_list, kld_list)\n",
    "\n",
    "    elbo_list = []\n",
    "    rec_term_list = []\n",
    "    kld_list = []\n",
    "    beta_list = []\n",
    "\n",
    "\n",
    "    for step, (x, _) in zip(range(n_steps_outer), infinite_val_loader()): \n",
    "        x = x.numpy()\n",
    "        inner_train_data = jnp.asarray(\n",
    "            list(map(lambda x:x[0].numpy(), list(itertools.islice(train_loader, n_steps_inner)))) # get the next n_steps_inner elements from train_loader\n",
    "        )\n",
    "        params, beta, states, opt_state_outer, opt_state_inner, key, x_rec, losses = make_step(\n",
    "            x, params, beta, states, inner_train_data, opt_state_outer, opt_state_inner, key\n",
    "        )\n",
    "        elbo_list.extend(-losses[0])\n",
    "        rec_term_list.extend(losses[1])\n",
    "        kld_list.extend(losses[2])\n",
    "        # elbo_list.append(-losses[0])\n",
    "        # rec_term_list.append(losses[1])\n",
    "        # kld_list.append(losses[2])\n",
    "        beta_list.append(jnp.full((n_steps_inner,), beta))\n",
    "        \n",
    "        if (step % print_every) == 0 or (step == n_steps - 1):\n",
    "            print(\n",
    "                f\"{step=}, elbo_loss={elbo_list[-1]}, rec_term={rec_term_list[-1]}, kld_term={kld_list[-1]}, beta={beta_list[-1][0]}\"\n",
    "            )\n",
    "            \n",
    "    return params, beta, states, (elbo_list, rec_term_list, kld_list), (opt_state_outer, opt_state_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "adcfb805-5c54-42fb-9b83-4e7845ff23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.tree.map(lambda x, y: jax.tree.map(lambda y_:x+y_, y, is_leaf=lambda x:(isinstance(x, float))),\n",
    "#              (0., 0., 0.),\n",
    "#              ((1., None), (2., None), (3., None, 4)),\n",
    "#              is_leaf=lambda x:(isinstance(x, float)))# and isinstance(y, tuple)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "540a810f-bd72-4da6-b317-5f9c555c9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 7, 7) (9408,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:53:13.909010: W external/tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 73.52GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv.1 = (f32[2,602112,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[2,3,256,256]{3,2,1,0} %Arg_73.74, f32[602112,3,7,7]{3,2,1,0} %bitcast.2732), window={size=7x7 stride=2x2 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"jit(loss)/jit(main)/vmap(eqx.nn.Conv)/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2, 3), rhs_spec=(0, 1, 2, 3), out_spec=(0, 1, 2, 3)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/resnet.py\" source_line=354}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 78936801280 bytes.\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m----> 2\u001b[0m final_params, final_beta, final_states, loss_lists, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_outer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_outer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_beta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state_outer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[127], line 257\u001b[0m, in \u001b[0;36mtrain_outer\u001b[0;34m(n_steps_outer, n_steps_inner, params, beta, static, states, train_loader, val_loader, opt_state_outer, opt_state_inner, key, print_every)\u001b[0m\n\u001b[1;32m    253\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    254\u001b[0m inner_train_data \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(train_loader, n_steps_inner)))) \u001b[38;5;66;03m# get the next n_steps_inner elements from train_loader\u001b[39;00m\n\u001b[1;32m    256\u001b[0m )\n\u001b[0;32m--> 257\u001b[0m params, beta, states, opt_state_outer, opt_state_inner, key, x_rec, losses \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state_outer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m elbo_list\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;241m-\u001b[39mlosses[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    261\u001b[0m rec_term_list\u001b[38;5;241m.\u001b[39mextend(losses[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[127], line 179\u001b[0m, in \u001b[0;36mtrain_outer.<locals>.make_step\u001b[0;34m(outer_val_batch, params, beta0, states, inner_train_data, opt_state_outer, opt_state_inner, key)\u001b[0m\n\u001b[1;32m    170\u001b[0m loss_wrapper_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p: loss_wrapper(\u001b[38;5;241m*\u001b[39mtrue_params_flattened[:\u001b[38;5;241m0\u001b[39m], p, \u001b[38;5;241m*\u001b[39mtrue_params_flattened[\u001b[38;5;241m0\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:], tree\u001b[38;5;241m=\u001b[39mtree)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# NOTE fails with JAX ERROR\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# print(jet.jet(\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m#     loss_wrapper_2,\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m#     (true_params_flattened[0],),\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m#     ((jnp.ones_like(true_params_flattened[0]), jnp.zeros_like(true_params_flattened[0])),)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# )[1][1])\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacfwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_wrapper_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_params_flattened\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m fs\n\u001b[1;32m    186\u001b[0m d2L_dtheta2_ \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/vae_grf/lib/python3.11/site-packages/jax/_src/api.py:810\u001b[0m, in \u001b[0;36mjacfwd.<locals>.jacfun\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[1;32m    809\u001b[0m   pushfwd: Callable \u001b[38;5;241m=\u001b[39m partial(_jvp, f_partial, dyn_args)\n\u001b[0;32m--> 810\u001b[0m   y, jac \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpushfwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_std_basis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdyn_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m   pushfwd: Callable \u001b[38;5;241m=\u001b[39m partial(_jvp, f_partial, dyn_args, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[127], line 180\u001b[0m, in \u001b[0;36mtrain_outer.<locals>.make_step.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    170\u001b[0m loss_wrapper_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p: loss_wrapper(\u001b[38;5;241m*\u001b[39mtrue_params_flattened[:\u001b[38;5;241m0\u001b[39m], p, \u001b[38;5;241m*\u001b[39mtrue_params_flattened[\u001b[38;5;241m0\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:], tree\u001b[38;5;241m=\u001b[39mtree)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# NOTE fails with JAX ERROR\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# print(jet.jet(\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m#     loss_wrapper_2,\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m#     (true_params_flattened[0],),\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m#     ((jnp.ones_like(true_params_flattened[0]), jnp.zeros_like(true_params_flattened[0])),)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# )[1][1])\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(jax\u001b[38;5;241m.\u001b[39mjacfwd(\n\u001b[0;32m--> 180\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m p:\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_wrapper_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    181\u001b[0m             argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    182\u001b[0m         )(true_params_flattened[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m fs\n\u001b[1;32m    186\u001b[0m d2L_dtheta2_ \u001b[38;5;241m=\u001b[39m []\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[127], line 170\u001b[0m, in \u001b[0;36mtrain_outer.<locals>.make_step.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_(p)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# # Note that we cannot have tracer for grad's argnum (https://stackoverflow.com/questions/77913154/vectorizing-power-of-jax-grad)\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# @partial(jax.jit, static_argnums=(1,))\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# def get_gradients(idx, nb_params_array):\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m#     \"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m#     Returns the idx gradient among the nb_params_array gradients that are precomputed at trace time\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m#     \"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m loss_wrapper_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mloss_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrue_params_flattened\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrue_params_flattened\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# NOTE fails with JAX ERROR\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# print(jet.jet(\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m#     loss_wrapper_2,\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m#     (true_params_flattened[0],),\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m#     ((jnp.ones_like(true_params_flattened[0]), jnp.zeros_like(true_params_flattened[0])),)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# )[1][1])\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(jax\u001b[38;5;241m.\u001b[39mjacfwd(\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m p:jax\u001b[38;5;241m.\u001b[39mgrad(loss_wrapper_2, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)(p),\n\u001b[1;32m    181\u001b[0m             argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    182\u001b[0m         )(true_params_flattened[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    183\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[127], line 162\u001b[0m, in \u001b[0;36mtrain_outer.<locals>.make_step.<locals>.loss_wrapper\u001b[0;34m(tree, *true_params_flattened)\u001b[0m\n\u001b[1;32m    154\u001b[0m true_params_unflattened \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x, y:x\u001b[38;5;241m.\u001b[39mreshape(y\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mlist\u001b[39m(true_params_flattened),\n\u001b[1;32m    157\u001b[0m     true_params,\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m#is_leaf=eqx.is_inexact_array\u001b[39;00m\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    160\u001b[0m p \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39munflatten(tree, true_params_unflattened)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[127], line 143\u001b[0m, in \u001b[0;36mtrain_outer.<locals>.make_step.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    140\u001b[0m true_params_flattened \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mflatten(), true_params, is_leaf\u001b[38;5;241m=\u001b[39meqx\u001b[38;5;241m.\u001b[39mis_inexact_array)\n\u001b[1;32m    141\u001b[0m nb_params_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(true_params)\n\u001b[0;32m--> 143\u001b[0m loss_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_val_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39mtrue_params_flattened, tree):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"To be able to differentiate the certain elements of the pytree\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    https://github.com/google/jax/discussions/12765\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    Note that all that goes after *args are keyword only arguments\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 31 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/vae_grf/lib/python3.11/site-packages/jax/_src/compiler.py:237\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    233\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv.1 = (f32[2,602112,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[2,3,256,256]{3,2,1,0} %Arg_73.74, f32[602112,3,7,7]{3,2,1,0} %bitcast.2732), window={size=7x7 stride=2x2 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"jit(loss)/jit(main)/vmap(eqx.nn.Conv)/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2, 3), rhs_spec=(0, 1, 2, 3), out_spec=(0, 1, 2, 3)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/resnet.py\" source_line=354}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 78936801280 bytes.\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning."
     ]
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "final_params, final_beta, final_states, loss_lists, _ = train_outer(\n",
    "    n_steps_outer, n_steps_inner, init_params, init_beta, static, init_states, train_dataloader, val_dataloader, opt_state_outer, opt_state_inner, key, print_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd0210-005d-4184-8453-833a5c9cc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_lists[0], label=\"elbo\")\n",
    "plt.plot(loss_lists[1], label=\"rec_term\")\n",
    "plt.plot(loss_lists[2], label=\"kld\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1caf0b9-fb30-4f98-81d4-8cee30886fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import LivestockTestDataset\n",
    "test_dataset = LivestockTestDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1024,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14631ec-9781-46d2-be62-697c939dd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = next(iter(test_dataloader))\n",
    "x_test =  x_test[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356df63e-7e99-4f83-93dd-645af7cdb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "(_, aux_loss) = loss(final_params, static, final_states, x_test, subkey, train=False, beta=beta)\n",
    "x_rec_test = aux_loss[0]\n",
    "\n",
    "vae_mu = aux_loss[-1]\n",
    "mad = jnp.mean(jnp.abs(vae_mu - jnp.mean(vae_mu, axis=(1), keepdims=True)), axis=(1)) # mean on latent dims for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bea16-caf1-41b4-9a63-6e857909165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 4)\n",
    "axes[0, 0].imshow(jnp.moveaxis(x_test[1],0, 2))\n",
    "axes[1, 0].imshow(jnp.moveaxis(x_rec_test[1], 0, 2))\n",
    "axes[0, 1].imshow(mad[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7c5661b1-99f2-4a88-893b-7bc21102d8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5033326148986816\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import jet\n",
    "# jet.fact = lambda n: jax.lax.prod(range(1, n + 1))\n",
    "\n",
    "def f(ws, wo, x):\n",
    "    for w in ws:\n",
    "        x = jax.lax.exp(x @ w)\n",
    "    return jnp.reshape(x @ wo, ())\n",
    "\n",
    "@jax.jit\n",
    "@partial(jax.vmap, in_axes=(None, None, 0))\n",
    "def laplacian_1(ws, wo, x):\n",
    "    fun = partial(f, ws, wo)\n",
    "    @jax.vmap\n",
    "    def hvv(v):\n",
    "        return jet.jet(fun, (x,), ((v, jnp.zeros_like(x)),))[1][1]\n",
    "    return jnp.sum(hvv(jnp.eye(x.shape[0], dtype=x.dtype)))\n",
    "\n",
    "@jax.jit\n",
    "@partial(jax.vmap, in_axes=(None, None, 0))\n",
    "def laplacian_2(ws, wo, x):\n",
    "    fun = partial(f, ws, wo)\n",
    "    in_tangents = jnp.eye(x.shape[0], dtype=x.dtype)\n",
    "    pushfwd = partial(jax.jvp, jax.grad(fun), (x,))\n",
    "    _, hessian = jax.vmap(pushfwd, out_axes=(None, 0))((in_tangents,))\n",
    "    return jnp.trace(hessian)\n",
    "\n",
    "@jax.jit\n",
    "@partial(jax.vmap, in_axes=(None, None, 0))\n",
    "def laplacian_3(ws, wo, x):\n",
    "    fun = partial(f, ws, wo)\n",
    "    return jnp.trace(jax.hessian(fun)(x))\n",
    "\n",
    "def timer(f):\n",
    "    from time import time\n",
    "    f() # compile\n",
    "    t = time()\n",
    "    for _ in range(3):\n",
    "        f()\n",
    "    print((time() - t) / 3)\n",
    "\n",
    "d = 256\n",
    "ws = [jnp.zeros((d, d)) for _ in range(3)]\n",
    "wo = jnp.zeros((d, 1))\n",
    "x = jnp.zeros((512, d))\n",
    "\n",
    "timer(lambda : jax.block_until_ready(laplacian_1(ws, wo, x)))\n",
    "# timer(lambda : jax.block_until_ready(laplacian_2(ws, wo, x)))\n",
    "# timer(lambda : jax.block_until_ready(laplacian_3(ws, wo, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe8dc7-8315-4539-8206-7df7622ebfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
