{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0081623-ddf7-4eea-9904-1237d1385270",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5515a157-74b4-423c-9910-643c6ef5898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import jit, vmap\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5c520e-e429-453d-b493-0e01d7c4bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"0.95\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b108b2a0-1b13-4611-ae2e-19ad8c80f8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c277e07d-1e90-4118-bc91-1dca1cf02b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ab576d-3fc4-4f49-a779-8cb123209ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 13:26:54.931660: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9197164e-9ae0-4615-8582-9b93f5c06ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0d50ae-425f-4f88-b4f9-0520b6ef759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae_jax import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb27bf5-75c3-4fe7-a89f-8943e2cc6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, vae_states = eqx.nn.make_with_state(VAE)(256, key)\n",
    "init_vae_params, vae_static = eqx.partition(vae, eqx.is_inexact_array)\n",
    "# x = jnp.zeros((1, 3, 256, 256)) # we must have a batch_dim, the model can only be applied afetr vmap because of BN layer\n",
    "# batch_model = jax.vmap(model, in_axes=(0, None, None, None), out_axes=(0, None), axis_name=\"batch\")\n",
    "# x, state = batch_model(x, state, key, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b136a3-1b10-4b5f-a358-87477ffcf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_train_dataloader, get_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77be5b34-6d1e-4bde-9da5-1a236a79e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2be38c-7611-4f9b-a4b7-d6f311bf79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import LivestockTrainDataset\n",
    "train_dataset = LivestockTrainDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=50000,\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aafa604-c820-494f-8233-ff958c0b6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(1, 6))\n",
    "def loss(params, static, states, x, key, beta, train=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        XXX\n",
    "    static\n",
    "        XXX\n",
    "    states\n",
    "        XXX\n",
    "    x\n",
    "        A batch of inputs\n",
    "    key\n",
    "        A JAX PRNGKey\n",
    "    \"\"\"\n",
    "    vae = eqx.combine(params, static)\n",
    "    if train:\n",
    "        # make sure we are in training mode\n",
    "        vae = eqx.nn.inference_mode(vae, value=False)\n",
    "    else:\n",
    "        vae = eqx.nn.inference_mode(vae)\n",
    "    batched_vae = vmap(vae, in_axes=(0, None, None, None), out_axes=(0,  None, 0, 0), axis_name=\"batch\")\n",
    "\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "\n",
    "    x_rec, states, mu, logvar = batched_vae(x, states, key, train)\n",
    "    print(x_rec)\n",
    "    batched_elbo = vmap(vae.elbo, in_axes=(0, 0, 0, 0, None), out_axes=(0, 0, 0))\n",
    "\n",
    "    elbo, rec_term, kld = batched_elbo(x_rec, x, mu, logvar, beta)\n",
    "    elbo = jnp.mean(elbo) # avg over the batches\n",
    "    rec_term = jnp.mean(rec_term)\n",
    "    kld = jnp.mean(kld)\n",
    "\n",
    "    #x_rec = VAE.mean_from_lambda(x_rec)\n",
    "\n",
    "    return -elbo, (x_rec, rec_term, kld, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fbc4f-f9d6-44f7-a5ce-d485dd10fa21",
   "metadata": {},
   "source": [
    "Test the loss on one mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "958301ff-6d72-41e2-a7cc-3403972b11a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(\n",
      "  0x87f3c8=bool[],\n",
      "  0x87f3e8=(f32[64], f32[64]),\n",
      "  0x87f408=bool[],\n",
      "  0x87f428=(f32[64], f32[64]),\n",
      "  0x87f448=bool[],\n",
      "  0x87f468=(f32[64], f32[64]),\n",
      "  0x87f488=bool[],\n",
      "  0x87f4a8=(f32[64], f32[64]),\n",
      "  0x87f4c8=bool[],\n",
      "  0x87f4e8=(f32[64], f32[64]),\n",
      "  0x87f508=bool[],\n",
      "  0x87f528=(f32[128], f32[128]),\n",
      "  0x87f548=bool[],\n",
      "  0x87f568=(f32[128], f32[128]),\n",
      "  0x87f588=bool[],\n",
      "  0x87f5a8=(f32[128], f32[128]),\n",
      "  0x87f5c8=bool[],\n",
      "  0x87f5e8=(f32[128], f32[128]),\n",
      "  0x87f608=bool[],\n",
      "  0x87f628=(f32[128], f32[128]),\n",
      "  0x87f648=bool[],\n",
      "  0x87f668=(f32[256], f32[256]),\n",
      "  0x87f688=bool[],\n",
      "  0x87f6a8=(f32[256], f32[256]),\n",
      "  0x87f6c8=bool[],\n",
      "  0x87f6e8=(f32[256], f32[256]),\n",
      "  0x87f708=bool[],\n",
      "  0x87f728=(f32[256], f32[256]),\n",
      "  0x87f748=bool[],\n",
      "  0x87f768=(f32[256], f32[256]),\n",
      "  0x87f788=bool[],\n",
      "  0x87f7a8=(f32[512], f32[512]),\n",
      "  0x87f7c8=bool[],\n",
      "  0x87f7e8=(f32[512], f32[512]),\n",
      "  0x87f808=bool[],\n",
      "  0x87f828=(f32[512], f32[512]),\n",
      "  0x87f848=bool[],\n",
      "  0x87f868=(f32[512], f32[512]),\n",
      "  0x87f888=bool[],\n",
      "  0x87f8a8=(f32[512], f32[512]),\n",
      "  0x87f8c8=bool[],\n",
      "  0x87f8e8=(f32[128], f32[128]),\n",
      "  0x87f908=bool[],\n",
      "  0x87f928=(f32[64], f32[64]),\n",
      "  0x87f948=bool[],\n",
      "  0x87f968=(f32[32], f32[32])\n",
      ")\n",
      "Traced<ShapedArray(float32[16,10,3,256,256])>with<DynamicJaxprTrace(level=1/0)>\n",
      "(10, 3, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 13:27:05.980566: W external/tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-04 13:27:06.994681: W external/tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-04 13:27:11.164617: W external/tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 10.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-04 13:27:13.725578: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng11{k2=0,k3=0} for conv (f32[160,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,64,131,131]{3,2,1,0}, f32[32,64,4,4]{3,2,1,0}, f32[32]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:13.889799: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.164320402s\n",
      "Trying algorithm eng11{k2=0,k3=0} for conv (f32[160,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,64,131,131]{3,2,1,0}, f32[32,64,4,4]{3,2,1,0}, f32[32]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:14.890004: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng35{k2=2,k3=0} for conv (f32[160,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,64,131,131]{3,2,1,0}, f32[32,64,4,4]{3,2,1,0}, f32[32]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:15.048219: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.158393916s\n",
      "Trying algorithm eng35{k2=2,k3=0} for conv (f32[160,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,64,131,131]{3,2,1,0}, f32[32,64,4,4]{3,2,1,0}, f32[32]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:16.048411: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng11{k2=1,k3=0} for conv (f32[160,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,64,131,131]{3,2,1,0}, f32[32,64,4,4]{3,2,1,0}, f32[32]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:16.825211: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.77695831s\n",
      "Trying algorithm eng11{k2=1,k3=0} for conv (f32[160,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,64,131,131]{3,2,1,0}, f32[32,64,4,4]{3,2,1,0}, f32[32]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:16.825256: W external/tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 10.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-04 13:27:17.825441: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[160,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,64,131,131]{3,2,1,0}, f32[32,64,4,4]{3,2,1,0}, f32[32]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:19.441168: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.615894628s\n",
      "Trying algorithm eng0{} for conv (f32[160,32,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,64,131,131]{3,2,1,0}, f32[32,64,4,4]{3,2,1,0}, f32[32]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:25.638650: W external/tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 20.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-04 13:27:26.638879: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng20{k2=0,k4=2,k5=1,k6=0,k7=0,k19=0} for conv (f32[160,3,256,256]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,32,259,259]{3,2,1,0}, f32[3,32,4,4]{3,2,1,0}, f32[3]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:27.163553: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.52485577s\n",
      "Trying algorithm eng20{k2=0,k4=2,k5=1,k6=0,k7=0,k19=0} for conv (f32[160,3,256,256]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,32,259,259]{3,2,1,0}, f32[3,32,4,4]{3,2,1,0}, f32[3]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:28.163773: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng18{k2=0,k4=3,k5=1,k6=0,k7=0} for conv (f32[160,3,256,256]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,32,259,259]{3,2,1,0}, f32[3,32,4,4]{3,2,1,0}, f32[3]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-06-04 13:27:28.739053: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.575444424s\n",
      "Trying algorithm eng18{k2=0,k4=3,k5=1,k6=0,k7=0} for conv (f32[160,3,256,256]{3,2,1,0}, u8[0]{0}) custom-call(f32[160,32,259,259]{3,2,1,0}, f32[3,32,4,4]{3,2,1,0}, f32[3]{0}), window={size=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    }
   ],
   "source": [
    "mini_batch = next(iter(train_dataloader))\n",
    "loss_value, (_, _, _, vae_states) = loss(init_vae_params, vae_static, vae_states, mini_batch[0].numpy(), key, 1., train=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e6587ff-424f-4877-8405-318f67c1212d",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bfb534d-0408-4a26-8178-5754202e4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "init_params = init_vae_params\n",
    "init_states = vae_states\n",
    "static = vae_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6cdcf9e-c250-4d95-8e81-ce1316cdb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = eqx.combine(init_params, static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af1394fd-26b4-480b-86b5-3d47e08001d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "n_steps = 100\n",
    "learning_rate = 1e-3\n",
    "print_every = 1\n",
    "beta = jnp.array(0.00001)\n",
    "\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(init_params)\n",
    "\n",
    "transition_steps = 100\n",
    "schedule_fn = optax.linear_schedule(init_value=0.1, end_value=10, transition_steps=transition_steps)\n",
    "beta_scheduler = optax.scale_by_schedule(schedule_fn)\n",
    "beta_sche_state = beta_scheduler.init(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "548bb682-1bad-4235-a474-6302c0f67cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_steps, params, static, states, train_loader, opt_state, beta, key, print_every, beta_sche_state=None):\n",
    "\n",
    "    def infinite_train_loader():\n",
    "        while True:\n",
    "            yield from train_loader\n",
    "\n",
    "    def make_step(x, params, states, opt_state, beta, beta_sche_state, i, key):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        if beta_sche_state is not None and i < transition_steps:\n",
    "            updates, beta_sche_state = beta_scheduler.update(jnp.ones_like(beta), beta_sche_state, beta)\n",
    "            beta = optax.apply_updates(beta, updates)\n",
    "        (minus_elbo, aux_loss), grads = jax.value_and_grad(loss, has_aux=True)(params, static, states, x, subkey, train=True, beta=beta)\n",
    "        x_rec, rec_term, kld, states = aux_loss\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, states, opt_state, key, x_rec, (minus_elbo, rec_term, kld), beta\n",
    "\n",
    "    elbo_list = []\n",
    "    rec_term_list = []\n",
    "    kld_list = []\n",
    "\n",
    "    beta_list = []\n",
    "\n",
    "\n",
    "    for step, (x, _) in zip(range(n_steps), infinite_train_loader()): \n",
    "        x = x.numpy()\n",
    "\n",
    "        params, states, opt_state, key, x_rec, losses, beta = make_step(x, params, states, opt_state, beta, beta_sche_state, step, key)\n",
    "        elbo_list.append(-losses[0])\n",
    "        rec_term_list.append(losses[1])\n",
    "        kld_list.append(losses[2])\n",
    "        beta_list.append(beta)\n",
    "        \n",
    "        if (step % print_every) == 0 or (step == n_steps - 1):\n",
    "            print(\n",
    "                f\"{step=}, elbo_loss={elbo_list[-1]}, rec_term={rec_term_list[-1]}, kld_term={kld_list[-1]}, beta={beta_list[-1]}\"\n",
    "            )\n",
    "        \n",
    "            \n",
    "    return params, states, (elbo_list, rec_term_list, kld_list), opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "540a810f-bd72-4da6-b317-5f9c555c9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(\n",
      "  0x87f3c8=bool[],\n",
      "  0x87f3e8=(f32[64], f32[64]),\n",
      "  0x87f408=bool[],\n",
      "  0x87f428=(f32[64], f32[64]),\n",
      "  0x87f448=bool[],\n",
      "  0x87f468=(f32[64], f32[64]),\n",
      "  0x87f488=bool[],\n",
      "  0x87f4a8=(f32[64], f32[64]),\n",
      "  0x87f4c8=bool[],\n",
      "  0x87f4e8=(f32[64], f32[64]),\n",
      "  0x87f508=bool[],\n",
      "  0x87f528=(f32[128], f32[128]),\n",
      "  0x87f548=bool[],\n",
      "  0x87f568=(f32[128], f32[128]),\n",
      "  0x87f588=bool[],\n",
      "  0x87f5a8=(f32[128], f32[128]),\n",
      "  0x87f5c8=bool[],\n",
      "  0x87f5e8=(f32[128], f32[128]),\n",
      "  0x87f608=bool[],\n",
      "  0x87f628=(f32[128], f32[128]),\n",
      "  0x87f648=bool[],\n",
      "  0x87f668=(f32[256], f32[256]),\n",
      "  0x87f688=bool[],\n",
      "  0x87f6a8=(f32[256], f32[256]),\n",
      "  0x87f6c8=bool[],\n",
      "  0x87f6e8=(f32[256], f32[256]),\n",
      "  0x87f708=bool[],\n",
      "  0x87f728=(f32[256], f32[256]),\n",
      "  0x87f748=bool[],\n",
      "  0x87f768=(f32[256], f32[256]),\n",
      "  0x87f788=bool[],\n",
      "  0x87f7a8=(f32[512], f32[512]),\n",
      "  0x87f7c8=bool[],\n",
      "  0x87f7e8=(f32[512], f32[512]),\n",
      "  0x87f808=bool[],\n",
      "  0x87f828=(f32[512], f32[512]),\n",
      "  0x87f848=bool[],\n",
      "  0x87f868=(f32[512], f32[512]),\n",
      "  0x87f888=bool[],\n",
      "  0x87f8a8=(f32[512], f32[512]),\n",
      "  0x87f8c8=bool[],\n",
      "  0x87f8e8=(f32[128], f32[128]),\n",
      "  0x87f908=bool[],\n",
      "  0x87f928=(f32[64], f32[64]),\n",
      "  0x87f948=bool[],\n",
      "  0x87f968=(f32[32], f32[32])\n",
      ")\n",
      "Traced<ShapedArray(float32[16,10,3,256,256])>with<DynamicJaxprTrace(level=3/0)>\n",
      "(10, 3, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 13:27:31.331088: W external/xla/xla/service/hlo_rematerialization.cc:2948] Can't reduce memory use below -1.35GiB (-1452923329 bytes) by rematerialization; only reduced to 6.43GiB (6904438181 bytes), down from 6.43GiB (6904438181 bytes) originally\n",
      "2024-06-04 13:27:41.906927: W external/tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 80.00MiB (rounded to 83886080)requested by op \n",
      "2024-06-04 13:27:41.907715: W external/tsl/tsl/framework/bfc_allocator.cc:494] ***************************************************************************************************x\n",
      "E0604 13:27:41.908344   57459 pjrt_stream_executor_client.cc:2826] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 83886080 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:   15.62MiB\n",
      "              constant allocation:         1B\n",
      "        maybe_live_out allocation:    5.03GiB\n",
      "     preallocated temp allocation:    1.28GiB\n",
      "  preallocated temp fragmentation:         0B (0.00%)\n",
      "                 total allocation:    6.33GiB\n",
      "              total fragmentation: 1003.42MiB (15.49%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 1.28GiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(eqx.nn.ConvTranspose))/reshape[new_sizes=(160, 32, 128, 128) dimensions=None]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=88\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[160,32,259,259]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 320.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,32,128,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 320.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[160,32,128,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 320.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,32,128,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 320.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,32,128,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 320.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(eqx.nn.BatchNorm))/sub\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=86\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[160,32,128,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 320.00MiB\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,32,128,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 160.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/add\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=98\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,256,32,32]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 160.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,64,64,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 160.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[160,64,64,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 160.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,64,64,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 160.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,64,64,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 160.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(eqx.nn.BatchNorm))/sub\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=83\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[160,64,64,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 160.00MiB\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,64,64,64]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 120.00MiB\n",
      "\t\tOperator: op_name=\"jit(loss)/jit(main)/reduce_sum[axes=(1, 2, 3, 4)]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=180\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[16,10,3,256,256]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 83886080 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   15.62MiB\n              constant allocation:         1B\n        maybe_live_out allocation:    5.03GiB\n     preallocated temp allocation:    1.28GiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:    6.33GiB\n              total fragmentation: 1003.42MiB (15.49%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 1.28GiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(eqx.nn.ConvTranspose))/reshape[new_sizes=(160, 32, 128, 128) dimensions=None]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=88\n\t\tXLA Label: fusion\n\t\tShape: f32[160,32,259,259]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,32,128,128]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n\t\tXLA Label: fusion\n\t\tShape: f32[160,32,128,128]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,32,128,128]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,32,128,128]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(eqx.nn.BatchNorm))/sub\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=86\n\t\tXLA Label: fusion\n\t\tShape: f32[160,32,128,128]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 320.00MiB\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,32,128,128]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/add\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=98\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,256,32,32]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,64,64,64]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n\t\tXLA Label: fusion\n\t\tShape: f32[160,64,64,64]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,64,64,64]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,64,64,64]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(eqx.nn.BatchNorm))/sub\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=83\n\t\tXLA Label: fusion\n\t\tShape: f32[160,64,64,64]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 160.00MiB\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,64,64,64]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 120.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/reduce_sum[axes=(1, 2, 3, 4)]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=180\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,3,256,256]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m----> 2\u001b[0m final_params, final_states, loss_lists, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_sche_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_steps, params, static, states, train_loader, opt_state, beta, key, print_every, beta_sche_state)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (x, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_steps), infinite_train_loader()): \n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 28\u001b[0m     params, states, opt_state, key, x_rec, losses, beta \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_sche_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     elbo_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39mlosses[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     30\u001b[0m     rec_term_list\u001b[38;5;241m.\u001b[39mappend(losses[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mtrain.<locals>.make_step\u001b[0;34m(x, params, states, opt_state, beta, beta_sche_state, i, key)\u001b[0m\n\u001b[1;32m     10\u001b[0m     updates, beta_sche_state \u001b[38;5;241m=\u001b[39m beta_scheduler\u001b[38;5;241m.\u001b[39mupdate(jnp\u001b[38;5;241m.\u001b[39mones_like(beta), beta_sche_state, beta)\n\u001b[1;32m     11\u001b[0m     beta \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(beta, updates)\n\u001b[0;32m---> 12\u001b[0m (minus_elbo, aux_loss), grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m x_rec, rec_term, kld, states \u001b[38;5;241m=\u001b[39m aux_loss\n\u001b[1;32m     14\u001b[0m updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state, params)\n",
      "    \u001b[0;31m[... skipping hidden 26 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/vae_grf/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1185\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1183\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1187\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 83886080 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   15.62MiB\n              constant allocation:         1B\n        maybe_live_out allocation:    5.03GiB\n     preallocated temp allocation:    1.28GiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:    6.33GiB\n              total fragmentation: 1003.42MiB (15.49%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 1.28GiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(eqx.nn.ConvTranspose))/reshape[new_sizes=(160, 32, 128, 128) dimensions=None]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=88\n\t\tXLA Label: fusion\n\t\tShape: f32[160,32,259,259]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,32,128,128]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n\t\tXLA Label: fusion\n\t\tShape: f32[160,32,128,128]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,32,128,128]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=87\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,32,128,128]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 320.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(eqx.nn.BatchNorm))/sub\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=86\n\t\tXLA Label: fusion\n\t\tShape: f32[160,32,128,128]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 320.00MiB\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,32,128,128]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/add\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=98\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,256,32,32]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,64,64,64]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n\t\tXLA Label: fusion\n\t\tShape: f32[160,64,64,64]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,64,64,64]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(jit(relu)))/max\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=84\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,64,64,64]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 160.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/vmap(vmap(eqx.nn.BatchNorm))/sub\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=83\n\t\tXLA Label: fusion\n\t\tShape: f32[160,64,64,64]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 160.00MiB\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,64,64,64]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 120.00MiB\n\t\tOperator: op_name=\"jit(loss)/jit(main)/reduce_sum[axes=(1, 2, 3, 4)]\" source_file=\"/home/hugo/Documents/writings/vae_grf_isprs/vae_grf/vae_jax.py\" source_line=180\n\t\tXLA Label: fusion\n\t\tShape: f32[16,10,3,256,256]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "final_params, final_states, loss_lists, opt_state = train(\n",
    "    n_steps, init_params, static, init_states, train_dataloader, opt_state, beta, key, print_every=print_every, beta_sche_state=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd0210-005d-4184-8453-833a5c9cc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_lists[0], label=\"elbo\")\n",
    "plt.plot(loss_lists[1], label=\"rec_term\")\n",
    "plt.plot(loss_lists[2], label=\"kld\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1caf0b9-fb30-4f98-81d4-8cee30886fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import LivestockTestDataset\n",
    "test_dataset = LivestockTestDataset(\n",
    "    img_size,\n",
    "    fake_dataset_size=1024,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e1487-a2e5-457e-bed4-8884e30abb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = next(iter(test_dataloader))\n",
    "x_test =  x_test[0].numpy()\n",
    "key, subkey = jax.random.split(key)\n",
    "(_, aux_loss) = loss(final_params, static, final_states, x_test, subkey, train=False, beta=beta)\n",
    "x_rec_test = aux_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bea16-caf1-41b4-9a63-6e857909165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 4)\n",
    "axes[0, 0].imshow(jnp.moveaxis(x_test[0],0, 2))\n",
    "axes[1, 0].imshow(jnp.moveaxis(x_rec_test[0], 0, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad7db2-3f25-45f9-ac37-06c8de9a3c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81170ade-1673-4aeb-ac9e-c39c622a3db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
